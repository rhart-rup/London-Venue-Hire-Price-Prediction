{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb83bebd-c803-4a0d-be3c-044de1fecd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import chromedriver_autoinstaller\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException, NoSuchElementException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99aa074-802f-4721-ad3a-128e748530e6",
   "metadata": {},
   "source": [
    "![alt text](images/tag_venue_home_page.png)\n",
    "# Tagvenue Venue Web Scrape\n",
    "### Introduction \n",
    "\n",
    "The [Tagvenue](https://www.tagvenue.com/) website is basically an Air BnB for finding and booking venues for an event. The website hosts thousands of venues in the UK that can be booked for events such as weddings, work drinks, birthdays etc. Each venue has one or more **spaces** available to be booked. A **space** is basically a room or area within the venue. Some venues have a single space, often the whole venue, whilst others offer a selection of rooms, each offered as a separate space. Each Space has its own webpage on Tagvenue. This webpage contains all the data needed to choose which space to book for your event. Example data includes price, location, size, capacity, features, licensing etc. This notebook will scrape the data from all spaces on the [Tagvenue](https://www.tagvenue.com/) website that are located in **London**. At the time of writing this amounts to **~4400** spaces. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944bcae5-183f-46df-9b4b-d81ec9206ade",
   "metadata": {},
   "source": [
    "### Key Variables\n",
    "The following key variables define and tweak the specifics of the web scrape: \n",
    "\n",
    "- **progress_report_interval** - Periodic progress reports (% completed) are printed during scraping. This variable defines in seconds how often the report is output. \n",
    "- **connection_error_retry_time** - This defines how long in seconds the program will wait before trying to re-load a webpage when it fails to load due to a connection error. \n",
    "- **headless_mode** - Set to *True* if you want chrome to be launched in headless mode i.e. not visible. Set to *False* if you wish chrome to be visible while scraping.  \n",
    "- **longitude_min**, **longitude_max**, **latitude_min** and **latitude_max** - Defines the area that will be searched for venues. The intersection of the four longitude / latitude lines defines a square area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98af76c8-8b80-4250-b9e5-cf3050f70bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_report_interval = 1800 #1800 for normal run, 300 for test\n",
    "connection_error_retry_time = 300  # 300 for normal run, 30 for test\n",
    "# Set True to have chrome open in headless mode \n",
    "headless_mode = False \n",
    "# longitude and latidue max and min define four lines, the intersection \n",
    "# of these lines defines a square area used for the venue search\n",
    "# Normal run values, comment out when not wanted \n",
    "latitude_min = 51.326626 \n",
    "latitude_max = 51.7297765\n",
    "longitude_min = -0.446500003\n",
    "longitude_max = 0.2190751\n",
    "# Test Values, comment out when not wanted \n",
    "#longitude_min = -0.100501\n",
    "#longitude_max = -0.059614\n",
    "#latitude_min = 51.494423\n",
    "#latitude_max = 51.50697"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23126e8-c587-4642-924c-5df6d2f4d03a",
   "metadata": {},
   "source": [
    "### Initiate Web Scraper\n",
    "We will use Selenium and Chromedriver / Chrome to crawl the Hire Space website and scrape data. An initial check is performed by *chromedriver_autoinstaller()* to ensure chromedriver is up to data. If it is not, then the latest version is downloaded. Selenium then initiates an instance of chrome that it can control. This instance will either be visible or invisible (headless mode) depending on the *headless_mode* variable.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e270f6a-24e0-4cf1-87dc-25a668bb7f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the current version of chromedriver exists\n",
    "# and if it doesn't exist, download it automatically,\n",
    "# then add chromedriver to path\n",
    "chromedriver_autoinstaller.install()\n",
    "# If headless_mode was True, open chrome in headless mode, \n",
    "# otherwise open a visible chrome browser\n",
    "if (headless_mode):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "else:\n",
    "    # Initialise chromedriver\n",
    "    driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ac69ac-b1fe-4712-8c38-8393943df19a",
   "metadata": {},
   "source": [
    "### Define Page Load Function\n",
    "We will frequently load new webpages with Selenium. We want to wait a certain amount of time between successive page loads to minimise our impact on the server and avoid being detected as a bot. We also want to detect any connection errors that might occur during the loading of a page for example due to a wifi issue. \n",
    "\n",
    "For this purpose, we created the *load_page* function. This function basically takes a url and loads it into chrome. It then pauses the program for twice the time it took for the page to load (with random variation to look less like a bot). In this way, the strain we put on the server will dynamically change. The slower the server becomes, the more time we will wait between successive page loads and vice versa if the server speeds up. \n",
    "\n",
    "The function also Handles *timeout* (page took longer than 30 seconds to load) and *connection* errors (couldn't connect to internet). In either case, the programme will wait some time then try to reload the page. If it still fails, the error is logged.  \n",
    "\n",
    "The function returns *True* if there were no errors loading the page and returns *False* when there were errors. It can be placed in an *if* statement so that the page will only be processed if the page loaded successfully.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6493ded9-c781-4c33-bf30-d89d7e88296a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set chromedriver timeout error to trigger if page takes more \n",
    "# than 30 seconds to load\n",
    "driver.set_page_load_timeout(30)\n",
    "# Initialise scraping error log\n",
    "scraping_error_log = []\n",
    "# Note - the below function returns True when no errors occur \n",
    "# during page load and is designed to be put within an 'if' \n",
    "# i.e. if(load_page(url)): to only do the steps in the 'if' \n",
    "# when the page load doesn't have errors\n",
    "def load_page(url):\n",
    "    \"\"\"Load provided url in chrome then sleep for interval of time. \n",
    "    \n",
    "    Handles and logs timout and connection errors. Calculates \n",
    "    the time to wait by multiplying the time it took the page to \n",
    "    load by 2, then adding some random offset.\n",
    "    \n",
    "    Returns -- True if page load was successful, returns False if there \n",
    "    was an error\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Loads url in chrome and calculates the time it took to load page\n",
    "        time_of_request = time.time()\n",
    "        driver.get(url)\n",
    "        page_load_time = time.time() - time_of_request\n",
    "        # Calculate time required to wait before next url is \n",
    "        # loaded\n",
    "        wait_time_till_next_request = wait_time_calculation(page_load_time)\n",
    "        time.sleep(wait_time_till_next_request)\n",
    "        # Returns True to indicate page load had no errors\n",
    "        return True\n",
    "    # Execution pauses if timeout or connection issue occurs \n",
    "    except (TimeoutException, WebDriverException) as e:\n",
    "        time.sleep(connection_error_retry_time)  \n",
    "        try:\n",
    "            # Loads url in chrome and calculates the time it took to load page\n",
    "            time_of_request = time.time()\n",
    "            driver.get(url)\n",
    "            page_load_time = time.time() - time_of_request\n",
    "            # Calculate time required to wait before next url is \n",
    "            # loaded (next time load_page is called)\n",
    "            wait_time_till_next_request = wait_time_calculation(page_load_time)\n",
    "            time.sleep(wait_time_till_next_request)\n",
    "        except TimeoutException:\n",
    "            scraping_error_log.append([url, \n",
    "                              'page failed to load, web page timed out'])\n",
    "            # Returns False to indicate page load had an error\n",
    "            return False\n",
    "        except WebDriverException:\n",
    "            scraping_error_log.append([url, \n",
    "                              'page failed to load, no internet connection'])\n",
    "            # Returns False to indicate page load had an error\n",
    "            return False\n",
    "            \n",
    "def wait_time_calculation(page_load_time):\n",
    "    \"\"\"Returns time required to wait before loading next url\n",
    "    \n",
    "    The wait time is 2 times the page_load_time, with \n",
    "    random variation\"\"\"\n",
    "    \n",
    "    average_wait_time = 2 * page_load_time\n",
    "    upper_wait_time = 1.33333 * average_wait_time\n",
    "    lower_wait_time = 0.77777 * average_wait_time\n",
    "    return random.uniform(lower_wait_time, upper_wait_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc1d286-fa8e-430a-9759-f98bddbdc572",
   "metadata": {},
   "source": [
    "### Creating Search URL\n",
    "![alt text](images/tag_venue_search_bar.png)\n",
    "We will use Tagvenue's [search page](https://www.tagvenue.com/) to find all venues located in London. The Tagvenue search requires an 'event type' to be chosen for the search. There are around **190** different 'event types' available to choose from. To find all venue's hosted by the website, we will have to repeat the search for all 190 available 'event types'. When a user clicks on the 'event type' field on the search page, a list of options is shown for them to choose from. Below we scrape the 'event type' options provided to the user.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ea6be51-ba54-4d3d-a8c8-166db834b031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 190 event types on Tagvenue\n"
     ]
    }
   ],
   "source": [
    "tagvenue_search_page_url = 'https://www.tagvenue.com/'\n",
    "if (load_page(tagvenue_search_page_url)):\n",
    "    # Find event type html input element\n",
    "    form_event_type_input = driver.find_element_by_xpath(\n",
    "        \"//input[@name='room_tag_autocomplete']\")\n",
    "    # Click on event type html input element - this loads the 'event \n",
    "    # type' html elements that contain the event type options into the\n",
    "    # webpage html\n",
    "    form_event_type_input.click()\n",
    "    # Find event type html elements \n",
    "    form_event_types_elements = driver.find_elements_by_xpath(\n",
    "        \"//div[@class='autocomplete-suggestions']//div\")\n",
    "    # Extract text from event types html elements \n",
    "    form_event_types = [element.get_attribute('innerHTML')\n",
    "                        for element in form_event_types_elements]\n",
    "    # replace spaces with '-', to make the event type conform to the \n",
    "    # url format used by Tagvenue - remove, from old approach \n",
    "    #event_types = [item.replace(' ','-') for item in event_types]\n",
    "else: raise Exception('page load error - cannot find event types')\n",
    "\n",
    "print(f\"There are {len(form_event_types)} event types on Tagvenue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109f9725-05a7-4efe-a86c-6a2bca059514",
   "metadata": {},
   "source": [
    "After you select an 'event type' and click search, a search url is created and loaded into chrome. You are then taken to the search results page. The 'event type' forms a sub-directory of the search url. An example search url is shown below, where the 'event type' chosen was '18th Birthday Party'.  \n",
    "\n",
    "https://www.tagvenue.com/uk/search/18th-birthday-party?location_id=6&people=&neighbourhood=London\n",
    "\n",
    "The text of the 'event type' options provided to the user when completing the form is not consistent with the text used in the url e.g. the event type *'Academic'* becomes *'academic-venues'* in the search url. As such, we will use selenium to perform a search using each 'event-type' option and will extract the text of the 'event-type' from the search url. This will enable us to create our own custom urls with custom longitude and latitude values later, rather than having to rely on the functionality of the search page.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5364d0ee-612e-462e-9c55-45fba282cb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_url_event_type(event_type):\n",
    "    \"\"\"Returns the event type text used in the search url for the\n",
    "    given event type\"\"\"\n",
    "    \n",
    "    load_page(tagvenue_search_page_url)\n",
    "    # Find event type input html element\n",
    "    event_type_input = driver.find_element_by_xpath(\n",
    "        \"//input[@name='room_tag_autocomplete']\")\n",
    "    # Input the event type\n",
    "    event_type_input.send_keys(event_type)\n",
    "    # Press enter\n",
    "    event_type_input.send_keys(Keys.ENTER)\n",
    "    # Find search button html element\n",
    "    search_button_element = driver.find_element_by_xpath(\n",
    "        \"//button[@class='c-button-cta c-button-cta--big js-hero-search']\")\n",
    "    # Click search button tom perform search\n",
    "    search_button_element.click()\n",
    "    # Wait to give search page time to load\n",
    "    time.wait(5)\n",
    "    # Extract search url used\n",
    "    search_url = driver.current_url\n",
    "    # Extract event type text from search url. Event type sits between\n",
    "    # the last '/' and first '?' in the search url.\n",
    "    end_of_event_type = search_url.find('?')\n",
    "    start_of_event_type = search_url.rfind('/', 0, end_of_event_type) + 1\n",
    "    return search_url[start_of_event_type : end_of_event_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b88f185c-4333-4d17-a72d-fb7b07b37e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore, left in for testing / debugging purposes\n",
    "#form_event_types = form_event_types[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5d0b237-089f-4c86-9c56-5049b69f96c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_event_types = []\n",
    "# Perform search on Tagvenue using each of the available event types\n",
    "# and extract the text used in the search url to denote the event type\n",
    "for form_event_type in form_event_types: \n",
    "    url_event_type = find_url_event_type(form_event_type)\n",
    "    url_event_types.append(url_event_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "00635eed-106f-419b-83ab-5d2cb0e6ae81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 search url event types\n"
     ]
    }
   ],
   "source": [
    "print(f\"Found {len(url_event_types)} search url event types\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ed724a23-6f10-4a79-bc8a-85bc303c9473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['18th-birthday-party',\n",
       " '30th-birthday-party',\n",
       " '40th-birthday-party',\n",
       " '50th-birthday-party',\n",
       " 'academic-venues']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show first 10 event types\n",
    "url_event_types[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3319ba4e-a704-4056-abaf-e7a837f14a79",
   "metadata": {},
   "source": [
    "We can build a custom search url from an event type and latitude and longitude maximum and minimum values. This url will return search results showing all spaces within the defined latitude and longitude range that are suitable for the event type chosen. We chose to use a custom url so that we had full control over the latitude and longitude ranges. If we had used the Tagvenue search page to generate a search url, we would have to rely on Tagvenue's definition of the area of London.   \n",
    "\n",
    "An example custom search url is shown below: \n",
    "\n",
    "https://www.tagvenue.com/uk/search/18th-birthday-party?longitude_from=-0.270&longitude_to=0.069&latitude_from=51.31&latitude_to=51.69&page=1\n",
    "\n",
    "We also include the page number in the url. The search results display 36 results per page. The page number defines which page of the results to show. We included this in our custom url because the Tagvenue website alters the url in Chrome after it shows you the results. This alteration changes the latitude and longitude values. As such, if you navigate to another page by selecting a page using the page navigation links at the bottom of the results, it will display spaces from the new longitude and latitude range, and not from the range in the original search url. As a result, when we want to navigate to a new page of the search results, we create a new search result url with an updated page number. This keeps the latitude and longitude range constant as we navigate through results pages.   \n",
    "\n",
    "Below, we define the function to create a custom search url from an event type and page number (note that the latitude and longitude ranges are defined in the Key Variables section above.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16425f04-c132-41aa-86c9-4bc3bc0b4290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tagvenues changes the longitude and latitude values of the url in \n",
    "# Chrome after you load the url, so you need to recreate the whole url\n",
    "# whenever you change page or event type to keep the results within \n",
    "# the desired longitude and latitude range. \n",
    "def create_search_url(event_type, page):\n",
    "    \"\"\"Build and return search url string\"\"\"\n",
    "    return f\"\"\"https://www.tagvenue.com/uk/search/{event_type}?\n",
    "           longitude_from={longitude_min}&longitude_to={longitude_max}\n",
    "           &latitude_from={latitude_min}&latitude_to={latitude_max}&page={page}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb46652c-e782-4c30-b678-98bc06a56db2",
   "metadata": {},
   "source": [
    "### Collect Space URLs From Search Results\n",
    "\n",
    "Using custom search urls, we can search for space's within our defined latitude and longitude ranges. The url will return a search results page as shown in the image below:  \n",
    "\n",
    "![alt text](images/tag_venue_search_results.png)\n",
    "\n",
    "Each space returned by the search is shown as a clickable picture. Clicking on the picture will take you to the web page of that space. We want to gather the urls of every space web page in the results. Once we have all the space urls, we can start scraping data from them.\n",
    "\n",
    "The results page will only show spaces which are suitable for the event type in the search url. As such, we will need to repeat the search for all ~190 event types to ensure that we find every space hosted on the website. The spaces will be duplicated in different search results i.e. sometimes the same space will appear for 'corporate event' and '18th Birthday party'. As such, we will need to remove duplicates at the end. We could have used a *set()* to hold the list of space urls, which would negate the need to remove duplicates at the end. However, it was determined that it was useful when reporting the progress of the scrape, to be able to count the total number of urls scraped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7f2237c-0776-4187-b96b-87aec2a22188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_total_results_pages():\n",
    "    \"\"\"Returns the number of pages of search results showing in Chrome\"\"\"\n",
    "    # Find pagination html elements - these create the clickable page\n",
    "    # numbers and arrows at bottom of search results page to naviagte \n",
    "    # through search results pages \n",
    "    pagination_elements = driver.find_elements_by_xpath(\n",
    "        \"//div[@class='results-pagination results-pagination--center']/ul/li/a\")\n",
    "    # Convert pagination elements to text values  \n",
    "    pagination = [element.get_attribute('innerHTML') \n",
    "                  for element in pagination_elements]\n",
    "    # If list is not empty i.e. len > 0 then return second last \n",
    "    # element - this is the total number of pages\n",
    "    if (len(pagination) > 1):\n",
    "        return int(pagination[-2])\n",
    "    # If list empty, then there is only one page, return 1\n",
    "    else: return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe282fa3-74ac-4818-aa66-8fcb875553df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block is just for creating a search url for debugging purposes \n",
    "#event_type = 'pop-up-event'\n",
    "#event_type = 'corporate-event'\n",
    "#page = 1\n",
    "#search_url = create_search_url('kids-partybus', page)\n",
    "#load_page(search_url) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "297701bd-c646-4232-9c0b-7029a690f445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_space_urls():\n",
    "    \"\"\"Returns the url of every space webpage result showing in Chrome.\"\"\"\n",
    "    # Find the html elements of the urls of spaces returned by the search\n",
    "    search_result_url_elements = driver.find_elements_by_xpath(\"//div[@class='v-search-results-items']/div/a\")\n",
    "    if(len(search_result_url_elements) == 0):\n",
    "        try:\n",
    "            no_search_results_message_element = driver.find_element_by_xpath(\"//h3\")\n",
    "            no_search_results_message = no_search_results_message_element.get_attribute('innerHTML')\n",
    "            no_search_results_message = no_search_results_message.replace(\"'\", \"\").lower().strip()\n",
    "            expected_message = ('sorry, we couldnt find any venues matching your criteria.')\n",
    "            if(no_search_results_message != expected_message):\n",
    "                scraping_error_log.append([search_url, 'search url failed'])\n",
    "        except NoSuchElementException: \n",
    "            scraping_error_log.append([search_url, 'search url failed'])\n",
    "\n",
    "    return [element.get_attribute('href') for element in search_result_url_elements]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a819b11-bda1-4ee2-9dc6-b81eafd969b2",
   "metadata": {},
   "source": [
    "- Need a tagvenue specific failed url test OOPs error!!! - need errors to include event_type\n",
    "- add progress complete and time taken bit\n",
    "- manually fix event type issue - or worse case use selenium to manually search through 190 different auto-complete options and add the correct bit..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10f238c-eeee-4bed-a6fa-d969941c5ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore, left in for testing / debugging purposes\n",
    "#event_types = event_types[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92922900-bc07-4581-b215-ad0cefb9f370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0003581047058105469\n",
      "Scraped 1 of 5 event_types\n",
      "Approximately 0.25 pages of search results scraped\n",
      "\n",
      "0.1997997760772705\n",
      "0.6479842662811279\n",
      "Scraped 2 of 5 event_types\n",
      "Approximately 2.138888888888889 pages of search results scraped\n",
      "\n",
      "0.4792027473449707\n",
      "0.5636961460113525\n",
      "Scraped 3 of 5 event_types\n",
      "Approximately 4.027777777777778 pages of search results scraped\n",
      "\n",
      "0.4968390464782715\n",
      "0.5072238445281982\n",
      "Scraped 4 of 5 event_types\n",
      "Approximately 5.916666666666667 pages of search results scraped\n",
      "\n",
      "0.4110429286956787\n"
     ]
    }
   ],
   "source": [
    "space_urls = []\n",
    "time_last_update = time.time()\n",
    "total_event_types = len(url_event_types)\n",
    "\n",
    "for event_number, event_type in enumerate(url_event_types): \n",
    "    if (time.time() - time_last_update > progress_report_interval):\n",
    "        print(f\"Scraped {event_number} of {total_event_types} event_types\")\n",
    "        pages_of_urls_scraped = len(space_urls)/36\n",
    "        print(f\"Approximately {pages_of_urls_scraped:0} pages of search results scraped\\n\")\n",
    "        time_last_update = time.time()\n",
    "    search_url = create_search_url(event_type, 1)\n",
    "    load_page(search_url)\n",
    "    total_pages = find_total_results_pages()\n",
    "    for current_page in range(1, total_pages + 1):\n",
    "        space_urls.extend(get_space_urls())\n",
    "        if(current_page < total_pages):\n",
    "            search_url = create_search_url(event_type, current_page + 1)\n",
    "            load_page(search_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3ab55d-9692-42a7-a6fa-cb3c15884572",
   "metadata": {},
   "source": [
    "note: if page fails to load then it will result in search url failed error as well - need to rerun whole search url and scrape all its pages rather than just redoing the pages that failed in case the 'total pages' calculation was incorrect due to page load error and calculated it as 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4956bcea-0a3d-4aaa-b4cf-492f8d051b63",
   "metadata": {},
   "source": [
    "### Scraping Error Log\n",
    "If the scraping of a venue was aborted due to a page load error, it is displayed below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "dd1275e5-a9dd-47a2-baaa-6ae912aefff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_0c985_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >url</th>\n",
       "      <th class=\"col_heading level0 col1\" >error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x10c8646a0>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_errors = pd.DataFrame(scraping_error_log, columns = ['url','error'])\n",
    "# Function to make urls clickable in jupyter\n",
    "def make_clickable(val):\n",
    "    return '<a href=\"{}\">{}</a>'.format(val,val)\n",
    "\n",
    "scrape_errors.style.format({'url': make_clickable})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "15c109dc-6012-46df-b984-56a0eb262a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5505 spaces to scrape\n"
     ]
    }
   ],
   "source": [
    "space_urls_unique = list(set(space_urls))\n",
    "print(f\"There are {len(space_urls_unique)} spaces to scrape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e7c824c3-7b35-4717-aac0-539d89bf2a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save space_urls_uniqe to file (as json)\n",
    "with open(\"space_urls.json\", 'w') as f:\n",
    "    # indent=2 is not needed but makes the file human-readable\n",
    "    json.dump(space_urls_unique, f, indent=2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe711b02-af93-4135-a8d4-20413f33ca48",
   "metadata": {},
   "source": [
    "If you wish to load a saved list of space urls, remove #s and run the below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7f42d7cd-1834-40e1-985a-f19ec05e902e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5505 spaces to scrape\n"
     ]
    }
   ],
   "source": [
    "#with open(\"space_urls.json\", 'r') as f:\n",
    "#    space_urls_unique = json.load(f)\n",
    "\n",
    "#print(f\"There are {len(space_urls_unique)} spaces to scrape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5443706e-34c2-41dc-a7f1-cc6b813bf42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All urls with a '?' are of form '?event-offer'\n",
      "Removed 1017 urls containing '?event-offer' \n",
      "There are now 4488 space urls\n"
     ]
    }
   ],
   "source": [
    "urls_df = pd.DataFrame(space_urls_unique)\n",
    "\n",
    "urls_with_qmark = urls_df[urls_df[0].str.contains('\\?')].shape[0]\n",
    "urls_with_qmark_event_offer = urls_df[urls_df[0].str.contains('\\?event-offer')].shape[0]\n",
    "if (urls_with_qmark == urls_with_qmark_event_offer): \n",
    "    print(f\"All urls with a '?' are of form '?event-offer'\" )\n",
    "else: \n",
    "    print(f\"There are urls with '?' not of the form '?event-offer'\")\n",
    "\n",
    "space_urls_cleaned = urls_df[~urls_df[0].str.contains('\\?event-offer')][0].to_list()\n",
    "print(f\"Removed {urls_with_qmark_event_offer} urls containing '?event-offer' \\nThere are now {len(space_urls_cleaned)} space urls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413e74b8-f934-4932-ad2a-a652d92186d0",
   "metadata": {},
   "source": [
    "Below shows all urls that contain a '?' but don't contain 'event-offer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d01e0c41-5b93-4a52-b8c5-bf99fbad6ba4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_clickable' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3z/dqb7xjp13pg6n8dk8d2gscy40000gn/T/ipykernel_37485/841293606.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mview\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murls_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murls_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\?'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m&\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0murls_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'event-offer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmake_clickable\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'make_clickable' is not defined"
     ]
    }
   ],
   "source": [
    "view = urls_df[(urls_df[0].str.contains('\\?'))&(~urls_df[0].str.contains('event-offer'))]\n",
    "view.style.format({0: make_clickable})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7921434e-a375-4db2-8e52-91103943bcd0",
   "metadata": {},
   "source": [
    "The below code is useful for searching through the venue urls to see the different venues and to find different packages available at the venues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "6cc8e9c2-cef3-4bb8-9f11-d37ef86a8974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_3c529_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_3c529_level0_row0\" class=\"row_heading level0 row0\" >280</th>\n",
       "      <td id=\"T_3c529_row0_col0\" class=\"data row0 col0\" ><a href=\"https://www.tagvenue.com/rooms/london/3903/tanner-warehouse/tanner-warehouse-courtyard\">https://www.tagvenue.com/rooms/london/3903/tanner-warehouse/tanner-warehouse-courtyard</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c529_level0_row1\" class=\"row_heading level0 row1\" >2055</th>\n",
       "      <td id=\"T_3c529_row1_col0\" class=\"data row1 col0\" ><a href=\"https://www.tagvenue.com/rooms/london/3308/tanner-warehouse/industrial-wedding\">https://www.tagvenue.com/rooms/london/3308/tanner-warehouse/industrial-wedding</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c529_level0_row2\" class=\"row_heading level0 row2\" >3639</th>\n",
       "      <td id=\"T_3c529_row2_col0\" class=\"data row2 col0\" ><a href=\"https://www.tagvenue.com/rooms/london/321/tanner-warehouse/tanner-warehouse\">https://www.tagvenue.com/rooms/london/321/tanner-warehouse/tanner-warehouse</a></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x127242c40>"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Switch between space_urls_unique and space_urls_cleaned to \n",
    "# get with and without packages, and to find specific venues \n",
    "# or venues with packages e.g. search for '\\?event-offer=wedding' \n",
    "# to get venues with wedding packages\n",
    "urls_df = pd.DataFrame(space_urls_unique)\n",
    "view = urls_df[urls_df[0].str.contains('tanner')]\n",
    "view.style.format({0: make_clickable})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fe2e76-806b-4eec-9608-479292bb76b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from operator import itemgetter\n",
    "local_vars = list(locals().items())\n",
    "# Size gives us variable size in Bytes\n",
    "size = [[var,sys.getsizeof(obj)] for var, obj in local_vars]\n",
    "size = sorted(size, key=itemgetter(1), reverse = True)\n",
    "for var, size in size:\n",
    "    print(var,f\"-> {size/1000000:,} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7758c0ee-4168-4e7f-88e8-b51ac232b203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4488"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(space_urls_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd1991e-ca78-4e82-b2cc-c0a5d5ce1c53",
   "metadata": {},
   "source": [
    "### Scrape Space Webpage HTML Code\n",
    "- click on Read all to load venue review breakdown score \n",
    "- error logging \n",
    "- save results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50c08a10-41a1-48ae-8ab7-16761cccff63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def click_read_all():\n",
    "    \"\"\"Clicks 'Read all' button to load breakdown of venue review score.  \n",
    "    \n",
    "    Each venue has an overall user review score. By clicking on \n",
    "    'Read all' you can see a breakdown of the overall review score \n",
    "    into 6 different scoring categories e.g. 'Catering', \n",
    "    'location', 'Value' etc. The function waits until the page has\n",
    "    finished loading to ensure all data has loaded before scraping the html\"\"\"\n",
    "    try:\n",
    "        # Finds 'Read all' button html element\n",
    "        read_all_element = driver.find_element_by_xpath(\n",
    "            \"//button[@class='c-button-link' and contains(text(),'Read all')]\")\n",
    "        read_all_element.click()\n",
    "    except NoSuchElementException:\n",
    "        # End function, no reviews so no 'Read all' button to click\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Wait until page finishes loading following the click \n",
    "        WebDriverWait(driver, 10).until(lambda d: d.execute_script('return document.readyState') == 'complete')\n",
    "    except TimeoutException: \n",
    "        scraping_error_log.append([url, 'failed to load user review score breakdowns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abee9f56-7bdc-4ae9-96a2-8ddfec464a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 382 of 4488 -> 8.5% Completed\n",
      "\n",
      "Scraped 1035 of 4488 -> 23.1% Completed\n",
      "\n",
      "Scraped 1672 of 4488 -> 37.3% Completed\n",
      "\n",
      "Scraped 2291 of 4488 -> 51.0% Completed\n",
      "\n",
      "Scraped 2898 of 4488 -> 64.6% Completed\n",
      "\n",
      "Scraped 3476 of 4488 -> 77.5% Completed\n",
      "\n",
      "Scraped 4034 of 4488 -> 89.9% Completed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dictionary storing the url and html of each space webpage, in \n",
    "# format {url:html} \n",
    "space_webpages = {}\n",
    "total_urls_to_scrape = len(space_urls_cleaned)\n",
    "time_last_update = time.time()\n",
    "\n",
    "# Loop through space urls and scrape the html code for each space web page\n",
    "for url_number, url in enumerate(space_urls_cleaned):\n",
    "    if (load_page(url)):\n",
    "        click_read_all()\n",
    "        space_webpages[url] = driver.page_source\n",
    "    if (time.time() - time_last_update > progress_report_interval):\n",
    "        perc_complete = url_number / total_urls_to_scrape\n",
    "        print(f\"Scraped {url_number} of {total_urls_to_scrape} -> {perc_complete:.1%} Completed\\n\")\n",
    "        time_last_update = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dba648e-9760-491c-90e0-707083a45216",
   "metadata": {},
   "source": [
    "### Errors During Space HTML Scraping\n",
    "If the scraping of a venue was aborted due to a page load error, it is displayed below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e1c3874-ed10-4cf8-a4c2-d0de7460d6e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_21a8f_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >url</th>\n",
       "      <th class=\"col_heading level0 col1\" >error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x10fb123a0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_errors = pd.DataFrame(scraping_error_log, columns = ['url','error'])\n",
    "# Function to make urls clickable in jupyter\n",
    "def make_clickable(val):\n",
    "    return '<a href=\"{}\">{}</a>'.format(val,val)\n",
    "\n",
    "scrape_errors.style.format({'url': make_clickable})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bffc4a0-c75f-4583-84d3-eae0b2c9e9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save space webpage html data to file \n",
    "with open('space_htmls.json', 'w') as fp:\n",
    "    json.dump(space_webpages, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a072bc39-0aef-412b-a861-a8509bf26c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if you wish to load space_htmls from file\n",
    "#with open('space_htmls.json', 'r') as fp:\n",
    "#    space_webpages = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dde0a7-b4d2-4b4a-aaa8-93b4091d42d8",
   "metadata": {},
   "source": [
    "### Extract Data From HTML\n",
    "We will use the lxml library to extract useful data from the html code of each space's website. The extraction code handles 2 specific issues detailed below: \n",
    "- Some spaces were no longer hosted by Hire Space and therefore had no data available. The code excludes these venues by checking whether the start of the header reads 'venue no longer hosted'. \n",
    "- Some web pages didn't load properly during scraping. They were missing data and trigger an error during extraction. The extraction code deals with them by re-loading the webpage and trying to extract data from the reloaded page's html.  \n",
    "Any other errors will be handled by aborting the extraction process and adding the error to an error log. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "31b96961-3d0d-46db-95c6-74db34571d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error log for extraction errors\n",
    "#extraction_errors = []\n",
    "\n",
    "def extract_data(url, html):\n",
    "    \"\"\"Returns extracted data from an element of raw_data. \n",
    "    \n",
    "    Each element of raw_data is in form [url, html]. This\n",
    "    function handles and logs errors during extraction and \n",
    "    sends the html to the extract_from_html function for extraction.\n",
    "    \n",
    "    Returns - Extracted data as a list or returns None if unable to \n",
    "    extract data\n",
    "    \"\"\"\n",
    "    \n",
    "    # check if the venue is no longer hosted\n",
    "    #if (check_page_not_hosted(website_data[1])):\n",
    "        # Log extraction error due to venue no longer hosted \n",
    "    #    extraction_errors.append([url, 'venue no longer hosted',\n",
    "     #                             website_data[1]])\n",
    "      #  return None\n",
    "    #try:\n",
    "        # Extract the data from the html code\n",
    "    return extract_from_html(url, html)\n",
    "    #except:\n",
    "        # If initial data extraction fails, re-load the url in chrome\n",
    "     #   load_page(url)\n",
    "      #  if \n",
    "       # try:\n",
    "            # Try to extract the data using the html code of the \n",
    "            # re-loaded page\n",
    "        #    return extract_from_html(url, driver.page_source)  \n",
    "        #except Exception as e:\n",
    "            # Log error, including error message in log\n",
    "         #   extraction_errors.append([url, e, website_data[1]])\n",
    "          #  return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "492c8fda-4fad-488d-b476-b2b54ad6de97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_element_by_xpath(xpath):\n",
    "    \"\"\"Returns first element found using provided xpath. \n",
    "    \n",
    "    If no element is found, an error is raised\"\"\"\n",
    "    # Finds all elements from provided Xpath as a list\n",
    "    elements = tree.xpath(xpath)\n",
    "    try:\n",
    "        return elements[0]\n",
    "    except IndexError:\n",
    "        raise IndexError('No element found via provided Xpath')\n",
    "    \n",
    "def find_elements_by_xpath(xpath):\n",
    "    \"\"\"Returns all elements found via provided xpath as a list\"\"\"\n",
    "    return tree.xpath(xpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6cf56129-f5c4-415d-9cff-d4c8ec739bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.tagvenue.com/rooms/london/19375/sizona/sizona-location')\n",
    "tree = etree.HTML(driver.page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ea2e8e9a-6e38-4236-9c12-d752ca899e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.527689479034 -0.093920230865479\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "af49c668-e257-437f-b51b-6b360a852847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "afdaad98-7e01-4e69-b616-f964958b9ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHEN EXPANDING FOR RICH DATA - test and run this function before the 'extract data' \n",
    "# function\n",
    "\n",
    "def extract_from_html(url, html):\n",
    "    \"\"\"Extracts and returns data from provided html.\"\"\"\n",
    "    # Parse html with lxml library\n",
    "    tree = etree.HTML(html)\n",
    "    \n",
    "    # Find h1 header html element that contains venue and space name \n",
    "    header_element = find_element_by_xpath(\"//h1\")\n",
    "    # Extract space and venue name string from html element\n",
    "    header = header_element.get('title').lower()\n",
    "    # header has general form 'space_name at venue_name'. Split venue\n",
    "    # and space name into a list using ' at ' as separator. \n",
    "    venue_and_space_name = header.split(' at ')\n",
    "    space_name = venue_and_space_name[0]\n",
    "    venue_name = venue_and_space_name[1]\n",
    "    # If list has more than 2 elements, an issue has occured\n",
    "    if (len(venue_and_space_name) > 2):\n",
    "        scraping_error_log.append(url, header\n",
    "                                  + '> error identifying space name and venue name')\n",
    "\n",
    "    # Find address html element\n",
    "    address_element = find_element_by_xpath(\n",
    "        \"//span[@class='c-room-header__text_link' and contains(text(),',')]\")\n",
    "    # Extract text, remove '\\n's and whitespace\n",
    "    address = address_element.text.replace('\\n','').strip()\n",
    "    \n",
    "    # Find html element for map\n",
    "    map_element = find_element_by_xpath(\"//a[@href='#map-modal']\")\n",
    "    # Extract latitude and convert to float\n",
    "    latitude = float(map_element.get('data-lat'))\n",
    "    # Extract longitude and convert to float\n",
    "    longitude = float(map_element.get('data-long'))\n",
    "\n",
    "    return [url, venue_name, space_name, latitude, longitude, address]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee2576c-8c2a-43cb-aa34-e84de72de16b",
   "metadata": {},
   "source": [
    "def check_page_not_hosted(website_html): \n",
    "    \"\"\"Check if venue no longer hosted, return result (True or False)\"\"\"\n",
    "    \n",
    "    # Parse html with lxml library\n",
    "    tree = etree.HTML(website_html)\n",
    "    # Find text of h1 heading\n",
    "    heading = tree.xpath(\"//h1/text()\")\n",
    "    # Ensure heading was returned (will return list length 0 if \n",
    "    # no heading found)\n",
    "    if (len(heading) > 0):\n",
    "        # Return true if heading matches expected heading \n",
    "        # for no longer hosted page\n",
    "        return (heading[0][:34] == 'Hire Space does not currently list')\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22f2bd77-8640-4248-8cd9-c5fcbd08524a",
   "metadata": {},
   "outputs": [],
   "source": [
    "space_webpages_test = {url:html for url, html in list(space_webpages.items())[0:10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a0c0079-1804-4be3-a5b3-c4ed4e892bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['https://www.tagvenue.com/rooms/london/7552/graeae-theatre-company/creative-hub', 'https://www.tagvenue.com/rooms/london/6679/city-cruises-ltd/christmas-cruises', 'https://www.tagvenue.com/rooms/london/7134/jamies-tudor-street/ground-floor-room', 'https://www.tagvenue.com/rooms/london/20299/the-fisheries/the-boardroom', 'https://www.tagvenue.com/rooms/london/6854/brewers-hall/livery-hall', 'https://www.tagvenue.com/rooms/london/8287/1-wimpole-street/ent-room', 'https://www.tagvenue.com/rooms/london/24727/studio-spaces/full-venue', 'https://www.tagvenue.com/rooms/london/2451/3-perseverance-works/studio', 'https://www.tagvenue.com/rooms/london/23541/cococure/basement', 'https://www.tagvenue.com/rooms/london/1358/grand-ballroom-at-the-montcalm/grand-ballroom'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space_webpages_test.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0a55b245-3905-4342-bc58-564acba23c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.tagvenue.com/rooms/london/7552/graeae-theatre-company/creative-hub',\n",
       " 'https://www.tagvenue.com/rooms/london/6679/city-cruises-ltd/christmas-cruises',\n",
       " 'https://www.tagvenue.com/rooms/london/7134/jamies-tudor-street/ground-floor-room',\n",
       " 'https://www.tagvenue.com/rooms/london/20299/the-fisheries/the-boardroom',\n",
       " 'https://www.tagvenue.com/rooms/london/6854/brewers-hall/livery-hall',\n",
       " 'https://www.tagvenue.com/rooms/london/8287/1-wimpole-street/ent-room',\n",
       " 'https://www.tagvenue.com/rooms/london/24727/studio-spaces/full-venue',\n",
       " 'https://www.tagvenue.com/rooms/london/2451/3-perseverance-works/studio',\n",
       " 'https://www.tagvenue.com/rooms/london/23541/cococure/basement',\n",
       " 'https://www.tagvenue.com/rooms/london/1358/grand-ballroom-at-the-montcalm/grand-ballroom']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space_urls_cleaned[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cb2c4381-7bd2-495c-bb24-8f41f398c80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 spaces were successfully scraped\n"
     ]
    }
   ],
   "source": [
    "# Extract data from the space webpages that were scraped\n",
    "data = [extract_data(url, html) for url, html in space_webpages_test.items()]\n",
    "# Remove None entries in data (These are from the erroneous \n",
    "# web pages that couldn't be scraped)\n",
    "#data = [item for item in data if (item != None)]\n",
    "print(f\"{len(data)} spaces were successfully scraped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a2f186-4b7d-404b-9911-26cab49c56d8",
   "metadata": {},
   "source": [
    "### Extraction Error Log\n",
    "If extraction failed on a webpage, display error below along with error message. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a2fdc9c4-1e8d-4922-8624-39523e9ae584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_ab80b_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >url</th>\n",
       "      <th class=\"col_heading level0 col1\" >error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1633f2f70>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_errors = pd.DataFrame(scraping_error_log, columns = ['url','error'])\n",
    "# Function to make urls clickable in jupyter\n",
    "def make_clickable(val):\n",
    "    return '<a href=\"{}\">{}</a>'.format(val,val)\n",
    "\n",
    "scrape_errors.style.format({'url': make_clickable})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5f1e2e-6195-45d8-a849-b009d8f71e68",
   "metadata": {},
   "source": [
    "### Summarise and Save Data\n",
    "Below we convert the extracted data to a dataframe, show first 10 rows and summary statistics. We then save the data to file as a csv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3b183875-12c2-467d-b288-757709a42e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>venue_name</th>\n",
       "      <th>space_name</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>address_line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.tagvenue.com/rooms/london/7552/gra...</td>\n",
       "      <td>sizona</td>\n",
       "      <td>sizona location</td>\n",
       "      <td>51.527689</td>\n",
       "      <td>-0.09392</td>\n",
       "      <td>Dingley Place, 4, London, EC1V 8BP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.tagvenue.com/rooms/london/6679/cit...</td>\n",
       "      <td>sizona</td>\n",
       "      <td>sizona location</td>\n",
       "      <td>51.527689</td>\n",
       "      <td>-0.09392</td>\n",
       "      <td>Dingley Place, 4, London, EC1V 8BP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.tagvenue.com/rooms/london/7134/jam...</td>\n",
       "      <td>sizona</td>\n",
       "      <td>sizona location</td>\n",
       "      <td>51.527689</td>\n",
       "      <td>-0.09392</td>\n",
       "      <td>Dingley Place, 4, London, EC1V 8BP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.tagvenue.com/rooms/london/20299/th...</td>\n",
       "      <td>sizona</td>\n",
       "      <td>sizona location</td>\n",
       "      <td>51.527689</td>\n",
       "      <td>-0.09392</td>\n",
       "      <td>Dingley Place, 4, London, EC1V 8BP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.tagvenue.com/rooms/london/6854/bre...</td>\n",
       "      <td>sizona</td>\n",
       "      <td>sizona location</td>\n",
       "      <td>51.527689</td>\n",
       "      <td>-0.09392</td>\n",
       "      <td>Dingley Place, 4, London, EC1V 8BP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url venue_name  \\\n",
       "0  https://www.tagvenue.com/rooms/london/7552/gra...     sizona   \n",
       "1  https://www.tagvenue.com/rooms/london/6679/cit...     sizona   \n",
       "2  https://www.tagvenue.com/rooms/london/7134/jam...     sizona   \n",
       "3  https://www.tagvenue.com/rooms/london/20299/th...     sizona   \n",
       "4  https://www.tagvenue.com/rooms/london/6854/bre...     sizona   \n",
       "\n",
       "        space_name  longitude  latitude                        address_line  \n",
       "0  sizona location  51.527689  -0.09392  Dingley Place, 4, London, EC1V 8BP  \n",
       "1  sizona location  51.527689  -0.09392  Dingley Place, 4, London, EC1V 8BP  \n",
       "2  sizona location  51.527689  -0.09392  Dingley Place, 4, London, EC1V 8BP  \n",
       "3  sizona location  51.527689  -0.09392  Dingley Place, 4, London, EC1V 8BP  \n",
       "4  sizona location  51.527689  -0.09392  Dingley Place, 4, London, EC1V 8BP  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data, columns = ['url','venue_name','space_name', 'latitude','longitude', 'address_line'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2e673e-7166-4c81-8f11-a2df2bd2ea42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f73970b-2ea7-40d2-9595-fc771b48fa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('hire_space_venue_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8b172a-1365-408b-80aa-68f47fe3f2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close chrome page\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c49253-d65a-4032-8564-6e703da3faae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from operator import itemgetter\n",
    "local_vars = list(locals().items())\n",
    "# Size gives us variable size in Bytes\n",
    "size = [[var,sys.getsizeof(obj)] for var, obj in local_vars]\n",
    "size = sorted(size, key=itemgetter(1), reverse = True)\n",
    "for var, size in size:\n",
    "    print(var,f\"-> {size/1000000:,} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b5fb90-4c35-48c1-bf16-b9991dfbf6f6",
   "metadata": {},
   "source": [
    "### Investigating Extraction Errors\n",
    "Below provides examples of using the extraction_errors list to re-create the errors during extraction to aid with debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9154001-fa72-40cc-9ad3-2cdb505b46aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts error message from first error \n",
    "Error_number = 0\n",
    "repr(extraction_errors[Error_number][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef63808-a451-4aef-aef8-8a7354007407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs extraction on erroneous html, returning full original error\n",
    "Error_number = 0\n",
    "extract_from_html('url', extraction_errors[Error_number][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2701eb-c9ed-4b11-b217-77ba85642a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_breakdown_title_elements = driver.find_elements_by_xpath(\n",
    "            \"//div[@class='reviews-modal-score__title' and contains(text(),'')]\")\n",
    "    review_breakdown_score_elements = driver.find_elements_by_xpath(\n",
    "            \"//div[@class='reviews-modal-score__score' and contains(text(),'')]\")\n",
    "    for element in review_breakdown_title_elements:\n",
    "        print(element.get_attribute('innerHTML'))\n",
    "        \n",
    "    for element in review_breakdown_score_elements:\n",
    "        print(element.get_attribute('innerHTML'))   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Venue_scrape",
   "language": "python",
   "name": "venue_scrape"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
