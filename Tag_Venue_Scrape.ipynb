{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb83bebd-c803-4a0d-be3c-044de1fecd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import chromedriver_autoinstaller\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException, NoSuchElementException\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99aa074-802f-4721-ad3a-128e748530e6",
   "metadata": {},
   "source": [
    "![alt text](images/tag_venue_home_page.png)\n",
    "# Tagvenue Venue Web Scrape\n",
    "### Introduction \n",
    "\n",
    "The [Tagvenue](https://www.tagvenue.com/) website is basically an Air BnB for finding and booking venues for an event. The website hosts thousands of venues in the UK that can be booked for events such as weddings, work drinks, birthdays etc. Each venue has one or more **spaces** available to be booked. A **space** is basically a room or area within the venue. Some venues have just a single space, often the whole venue, whilst others offer a selection of rooms, each offered as a separate space. Each Space has its own webpage on Tagvenue. This webpage contains all the data needed to choose which space to book for your event. Example data includes price, location, size, capacity, features, licensing etc. This notebook will scrape the data from all spaces on the [Tagvenue](https://www.tagvenue.com/) website that are located in **London**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944bcae5-183f-46df-9b4b-d81ec9206ade",
   "metadata": {},
   "source": [
    "### Key Variables\n",
    "The following key variables define and tweak the specifics of the web scrape: \n",
    "\n",
    "- **progress_report_interval** - Periodic progress reports (% completed) are printed during scraping. This variable defines in seconds how often the report is output. \n",
    "- **connection_error_retry_time** - This defines how long in seconds the program will wait before trying to re-load a webpage when it fails to load due to a connection error. \n",
    "- **headless_mode** - Set to *True* if you want chrome to be launched in headless mode i.e. not visible. Set to *False* if you wish chrome to be visible while scraping.  \n",
    "- **longitude_min**, **longitude_max**, **latitude_min** and **latitude_max** - Defines the area that will be searched for venues. The intersection of the four longitude / latitude lines defines a square area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98af76c8-8b80-4250-b9e5-cf3050f70bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_report_interval = 1800 #1800 for normal run, 300 for test\n",
    "connection_error_retry_time = 300  # 300 for normal run, 30 for test\n",
    "# Set True to have chrome open in headless mode \n",
    "headless_mode = False \n",
    "# longitude and latidue max and min define four lines, the intersection \n",
    "# of these lines defines a square area used for the venue search\n",
    "# Normal run values, comment out when not wanted \n",
    "latitude_min = 51.326626 \n",
    "latitude_max = 51.7297765\n",
    "longitude_min = -0.446500003\n",
    "longitude_max = 0.2190751\n",
    "# Test Values, comment out when not wanted \n",
    "#longitude_min = -0.100501\n",
    "#longitude_max = -0.059614\n",
    "#latitude_min = 51.494423\n",
    "#latitude_max = 51.50697"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23126e8-c587-4642-924c-5df6d2f4d03a",
   "metadata": {},
   "source": [
    "### Initiate Web Scraper\n",
    "We will use Selenium and Chromedriver / Chrome to crawl the Hire Space website and download data. An initial check is performed by *chromedriver_autoinstaller()* to ensure chromedriver is up to data. If it is not, then the latest version is downloaded. Selenium then initiates an instance of chrome that it can control. This instance will either be visible or invisible (headless mode) depending on the *headless_mode* variable.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e270f6a-24e0-4cf1-87dc-25a668bb7f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the current version of chromedriver exists\n",
    "# and if it doesn't exist, download it automatically,\n",
    "# then add chromedriver to path\n",
    "chromedriver_autoinstaller.install()\n",
    "# If headless_mode was True, open chrome in headless mode, \n",
    "# otherwise open a visible chrome browser\n",
    "if (headless_mode):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "else:\n",
    "    # Initialise chromedriver\n",
    "    driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ac69ac-b1fe-4712-8c38-8393943df19a",
   "metadata": {},
   "source": [
    "### Define Page Load Function\n",
    "We will frequently load new webpages with Selenium. We want to wait a certain amount of time between successive page loads to minimise our impact on the server and avoid being detected as a bot. We also want to detect any connection errors that might occur during the loading of a page for example due to a wifi issue. For this purpose, we created the *load_page* function. This function basically takes a url then loads it into chrome once enough time has passed since the last webpage was loaded in chrome. \n",
    "\n",
    "The function has some extra features detailed below: \n",
    "- Handles *timeout* (page took longer than 30 seconds to load) and *connection* errors (couldn't connect to internet). In either case, the programme will wait some time then try to reload the page. If it still fails, the error is logged and any data scraping for that page is aborted.  \n",
    "- Measures time it took for last page to load and then waits double this time (with a small random offset to appear less bot like) before the next page is loaded. This dynamically increases or decreases the frequency of requests according to how well the webserver is coping, ensuring we don't overwhelm it.   \n",
    "- It only needs to take a single argument, the url you wish to load. However, it can take a second argument to assist with error reporting when the url you are loading did not come directly from the Hire Space search results. In other words, if you clicked on a search result and on the subsequent page you clicked on a link, it can be useful to connect this latest link to the original hire space search result url. This can be done by including the Hire Space search result url as the second argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6493ded9-c781-4c33-bf30-d89d7e88296a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set chromedriver timeout error to trigger if page takes more \n",
    "# than 30 seconds to load\n",
    "driver.set_page_load_timeout(30)\n",
    "# Initialise error log for page loading\n",
    "scraping_error_log = []\n",
    "# Note - the below function returns True when no errors occur \n",
    "# during page load and is designed to be put within an 'if' \n",
    "# i.e. if(load_page(url)): to only do the steps in the 'if' \n",
    "# when the page load doesn't have errors\n",
    "def load_page(url):\n",
    "    \"\"\"Load provided url in chrome then sleep for interval of time. \n",
    "    \n",
    "    Handles and logs timout and connection errors. Calculates \n",
    "    the time to wait by multiplying the time it took the page to \n",
    "    load by 2, then adding some random offset.\n",
    "    \n",
    "    keyword arguments: \n",
    "    url -- url you wish to load in chrome\n",
    "    main_url -- Used for error logging. Provide the search results url\n",
    "    whilst scraping spaces within a venue. \n",
    "    \n",
    "    Returns -- True if page load was successful, returns False if there \n",
    "    was an error\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Loads url in chrome and calculates the time it took to load page\n",
    "        time_of_request = time.time()\n",
    "        driver.get(url)\n",
    "        page_load_time = time.time() - time_of_request\n",
    "        # Calculate time required to wait before next url is \n",
    "        # loaded\n",
    "        wait_time_till_next_request = wait_time_calculation(page_load_time)\n",
    "        time.sleep(wait_time_till_next_request)\n",
    "        # Returns True to indicate page load had no errors\n",
    "        return True\n",
    "    # Execution pauses if timeout or connection issue occurs \n",
    "    except (TimeoutException, WebDriverException) as e:\n",
    "        time.sleep(connection_error_retry_time)  \n",
    "        try:\n",
    "            # Loads url in chrome and calculates the time it took to load page\n",
    "            time_of_request = time.time()\n",
    "            driver.get(url)\n",
    "            page_load_time = time.time() - time_of_request\n",
    "            # Calculate time required to wait before next url is \n",
    "            # loaded (next time load_page is called)\n",
    "            wait_time_till_next_request = wait_time_calculation(page_load_time)\n",
    "            time.sleep(wait_time_till_next_request)\n",
    "        except TimeoutException:\n",
    "            scraping_error_log.append([url, \n",
    "                              'page failed to load, web page timed out'])\n",
    "            # Returns False to indicate page load had an error\n",
    "            return False\n",
    "        except WebDriverException:\n",
    "            scraping_error_log.append([url, \n",
    "                              'page failed to load, no internet connection'])\n",
    "            # Returns False to indicate page load had an error\n",
    "            return False\n",
    "            \n",
    "def wait_time_calculation(page_load_time):\n",
    "    \"\"\"Returns time required to wait before loading next url\n",
    "    \n",
    "    The wait time is 2 times the page_load_time, with \n",
    "    random variation\"\"\"\n",
    "    \n",
    "    average_wait_time = 2 * page_load_time\n",
    "    upper_wait_time = 1.33333 * average_wait_time\n",
    "    lower_wait_time = 0.77777 * average_wait_time\n",
    "    return random.uniform(lower_wait_time, upper_wait_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc1d286-fa8e-430a-9759-f98bddbdc572",
   "metadata": {},
   "source": [
    "### Search for Venues\n",
    "We will use Tagvenue's [search page](https://www.tagvenue.com/) to find all venues located in London. The Tagvenue search requires an 'event type' to be chosen for the search. There are around **190** different 'event types' available to choose from. To find all venue's in London, we will have to repeat the search for all 190 available 'event types'. Below  we scrape the 'event type' options from the Tagvenue [search page](https://www.tagvenue.com/).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ea6be51-ba54-4d3d-a8c8-166db834b031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 190 event types on Tagvenue\n"
     ]
    }
   ],
   "source": [
    "tagvenue_search_page_url = 'https://www.tagvenue.com/'\n",
    "if (load_page(tagvenue_search_page_url)):\n",
    "    # Find event type html input element\n",
    "    form_event_type_input = driver.find_element_by_xpath(\"//input[@name='room_tag_autocomplete']\")\n",
    "    # Click on event type html input element - this loads the 'event \n",
    "    # type' html elements that contain the event type options into the\n",
    "    # webpage html\n",
    "    form_event_type_input.click()\n",
    "    # Find event type html elements \n",
    "    form_event_types_elements = driver.find_elements_by_xpath(\"//div[@class='autocomplete-suggestions']//div\")\n",
    "    # Extract text from event types html elements \n",
    "    form_event_types = [element.get_attribute('innerHTML')for element in form_event_types_elements]\n",
    "    # replace spaces with '-', to make the event type conform to the \n",
    "    # url format used by Tagvenue - remove, from old approach \n",
    "    #event_types = [item.replace(' ','-') for item in event_types]\n",
    "else: raise Exception('page load error - cannot find event types')\n",
    "\n",
    "print(f\"There are {len(form_event_types)} event types on Tagvenue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109f9725-05a7-4efe-a86c-6a2bca059514",
   "metadata": {},
   "source": [
    "Not all event types are input in the url in the same way as shown in th auto-completion list. We correct these differences here manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5364d0ee-612e-462e-9c55-45fba282cb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_url_event_type(event_type):\n",
    "    load_page(tagvenue_search_page_url)\n",
    "    event_type_input = driver.find_element_by_xpath(\"//input[@name='room_tag_autocomplete']\")\n",
    "    event_type_input.send_keys(event_type)\n",
    "    event_type_input.send_keys(Keys.ENTER)\n",
    "    search_button_element = driver.find_element_by_xpath(\"//button[@class='c-button-cta c-button-cta--big js-hero-search']\")\n",
    "    search_button_element.click()\n",
    "    time.wait(5)\n",
    "    search_url = driver.current_url\n",
    "    end_of_event_type = search_url.find('?')\n",
    "    start_of_event_type = search_url.rfind('/', 0, end_of_event_type) + 1\n",
    "    return search_url[start_of_event_type : end_of_event_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b88f185c-4333-4d17-a72d-fb7b07b37e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore, left in for testing / debugging purposes\n",
    "#form_event_types = form_event_types[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5d0b237-089f-4c86-9c56-5049b69f96c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_event_types = []\n",
    "for form_event_type in form_event_types: \n",
    "    url_event_type = find_url_event_type(form_event_type)\n",
    "    url_event_types.append(url_event_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00635eed-106f-419b-83ab-5d2cb0e6ae81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(url_event_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed724a23-6f10-4a79-bc8a-85bc303c9473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['18th-birthday-party',\n",
       " '30th-birthday-party',\n",
       " '40th-birthday-party',\n",
       " '50th-birthday-party',\n",
       " 'academic-venues']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_event_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16425f04-c132-41aa-86c9-4bc3bc0b4290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tagvenues changes the longitude and latitude values of the url in \n",
    "# Chrome after you load the url, so you need to recreate the whole url\n",
    "# whenever you change page or event type to keep the results within \n",
    "# the desired longitude and latitude range. \n",
    "def create_search_url(event_type, page):\n",
    "    \"\"\"Build and return search url string\"\"\"\n",
    "    return f\"\"\"https://www.tagvenue.com/uk/search/{event_type}?\n",
    "           longitude_from={longitude_min}&longitude_to={longitude_max}\n",
    "           &latitude_from={latitude_min}&latitude_to={latitude_max}&page={page}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb46652c-e782-4c30-b678-98bc06a56db2",
   "metadata": {},
   "source": [
    "We will incorporate the latitude, longitude and radius variables from above to define a url that will return all London venues in the search results. We then load this url into chrome, allowing us to see the search results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7f2237c-0776-4187-b96b-87aec2a22188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_total_results_pages():\n",
    "    \"\"\"Returns the number of pages of search results showing in Chrome\"\"\"\n",
    "    # Find pagination html elements - these create the clickable page\n",
    "    # numbers and arrows at bottom of search results page to naviagte \n",
    "    # through search results pages \n",
    "    pagination_elements = driver.find_elements_by_xpath(\"//div[@class='results-pagination results-pagination--center']/ul/li/a\")\n",
    "    # Convert pagination elements to text values  \n",
    "    pagination = [element.get_attribute('innerHTML') for element in pagination_elements]\n",
    "    # If list is not empty i.e. len > 0 then return second last \n",
    "    # element - this is the total number of pages\n",
    "    if (len(pagination) > 1):\n",
    "        return int(pagination[-2])\n",
    "    # If list empty, then there is only one page, return 1\n",
    "    else: return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe282fa3-74ac-4818-aa66-8fcb875553df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block is just for creating a search url for debugging purposes \n",
    "event_type = 'pop-up-event'\n",
    "#event_type = 'corporate-event'\n",
    "page = 1\n",
    "search_url = create_search_url('kids-partybus', page)\n",
    "load_page(search_url) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "297701bd-c646-4232-9c0b-7029a690f445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_space_urls():\n",
    "    # Find the html elements of the urls of spaces returned by the search\n",
    "    search_result_url_elements = driver.find_elements_by_xpath(\"//div[@class='v-search-results-items']/div/a\")\n",
    "    if(len(search_result_url_elements) == 0):\n",
    "        try:\n",
    "            no_search_results_message_element = driver.find_element_by_xpath(\"//h3\")\n",
    "            no_search_results_message = no_search_results_message_element.get_attribute('innerHTML')\n",
    "            no_search_results_message = no_search_results_message.replace(\"'\", \"\").lower().strip()\n",
    "            expected_message = ('sorry, we couldnt find any venues matching your criteria.')\n",
    "            if(no_search_results_message != expected_message):\n",
    "                scraping_error_log.append([search_url, 'search url failed'])\n",
    "        except NoSuchElementException: \n",
    "            scraping_error_log.append([search_url, 'search url failed'])\n",
    "\n",
    "    return [element.get_attribute('href') for element in search_result_url_elements]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a819b11-bda1-4ee2-9dc6-b81eafd969b2",
   "metadata": {},
   "source": [
    "- Need a tagvenue specific failed url test OOPs error!!! - need errors to include event_type\n",
    "- add progress complete and time taken bit\n",
    "- manually fix event type issue - or worse case use selenium to manually search through 190 different auto-complete options and add the correct bit..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10f238c-eeee-4bed-a6fa-d969941c5ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore, left in for testing / debugging purposes\n",
    "#event_types = event_types[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92922900-bc07-4581-b215-ad0cefb9f370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0003581047058105469\n",
      "Scraped 1 of 5 event_types\n",
      "Approximately 0.25 pages of search results scraped\n",
      "\n",
      "0.1997997760772705\n",
      "0.6479842662811279\n",
      "Scraped 2 of 5 event_types\n",
      "Approximately 2.138888888888889 pages of search results scraped\n",
      "\n",
      "0.4792027473449707\n",
      "0.5636961460113525\n",
      "Scraped 3 of 5 event_types\n",
      "Approximately 4.027777777777778 pages of search results scraped\n",
      "\n",
      "0.4968390464782715\n",
      "0.5072238445281982\n",
      "Scraped 4 of 5 event_types\n",
      "Approximately 5.916666666666667 pages of search results scraped\n",
      "\n",
      "0.4110429286956787\n"
     ]
    }
   ],
   "source": [
    "space_urls = []\n",
    "time_last_update = time.time()\n",
    "total_event_types = len(url_event_types)\n",
    "\n",
    "for event_number, event_type in enumerate(url_event_types): \n",
    "    if (time.time() - time_last_update > progress_report_interval):\n",
    "        print(f\"Scraped {event_number} of {total_event_types} event_types\")\n",
    "        pages_of_urls_scraped = len(space_urls)/36\n",
    "        print(f\"Approximately {pages_of_urls_scraped:0} pages of search results scraped\\n\")\n",
    "        time_last_update = time.time()\n",
    "    search_url = create_search_url(event_type, 1)\n",
    "    load_page(search_url)\n",
    "    total_pages = find_total_results_pages()\n",
    "    for current_page in range(1, total_pages + 1):\n",
    "        space_urls.extend(get_space_urls())\n",
    "        if(current_page < total_pages):\n",
    "            search_url = create_search_url(event_type, current_page + 1)\n",
    "            load_page(search_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3ab55d-9692-42a7-a6fa-cb3c15884572",
   "metadata": {},
   "source": [
    "note: if page fails to load then it will result in search url failed error as well - need to rerun whole search url and scrape all its pages rather than just redoing the pages that failed in case the 'total pages' calculation was incorrect due to page load error and calculated it as 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "dd1275e5-a9dd-47a2-baaa-6ae912aefff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_0c985_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >url</th>\n",
       "      <th class=\"col_heading level0 col1\" >error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x10c8646a0>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_errors = pd.DataFrame(scraping_error_log, columns = ['url','error'])\n",
    "# Function to make urls clickable in jupyter\n",
    "def make_clickable(val):\n",
    "    return '<a href=\"{}\">{}</a>'.format(val,val)\n",
    "\n",
    "scrape_errors.style.format({'url': make_clickable})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "15c109dc-6012-46df-b984-56a0eb262a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5505 spaces to scrape\n"
     ]
    }
   ],
   "source": [
    "space_urls_unique = list(set(space_urls))\n",
    "print(f\"There are {len(space_urls_unique)} spaces to scrape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e7c824c3-7b35-4717-aac0-539d89bf2a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save space_urls_uniqe to file (as json)\n",
    "with open(\"space_urls.json\", 'w') as f:\n",
    "    # indent=2 is not needed but makes the file human-readable\n",
    "    json.dump(space_urls_unique, f, indent=2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe711b02-af93-4135-a8d4-20413f33ca48",
   "metadata": {},
   "source": [
    "If you wish to load a saved list of space urls, remove #s and run the below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "7f42d7cd-1834-40e1-985a-f19ec05e902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"space_urls.json\", 'r') as f:\n",
    "#    space_urls_unique = json.load(f)\n",
    "\n",
    "#print(f\"There are {len(space_urls_unique)} spaces to scrape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "5443706e-34c2-41dc-a7f1-cc6b813bf42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All urls with a '?' are of form '?event-offer'\n",
      "Removed 1017 urls containing '?event-offer' \n",
      "There are now 4488 space urls\n"
     ]
    }
   ],
   "source": [
    "urls_df = pd.DataFrame(space_urls_unique)\n",
    "\n",
    "urls_with_qmark = urls_df[urls_df[0].str.contains('\\?')].shape[0]\n",
    "urls_with_qmark_event_offer = urls_df[urls_df[0].str.contains('\\?event-offer')].shape[0]\n",
    "if (urls_with_qmark == urls_with_qmark_event_offer): \n",
    "    print(f\"All urls with a '?' are of form '?event-offer'\" )\n",
    "else: \n",
    "    print(f\"There are urls with '?' not of the form '?event-offer'\")\n",
    "\n",
    "space_urls_cleaned = urls_df[~urls_df[0].str.contains('\\?event-offer')].values.tolist()\n",
    "print(f\"Removed {urls_with_qmark_event_offer} urls containing '?event-offer' \\nThere are now {len(space_urls_cleaned)} space urls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413e74b8-f934-4932-ad2a-a652d92186d0",
   "metadata": {},
   "source": [
    "Below shows all urls that contain a '?' but don't contain 'event-offer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "d01e0c41-5b93-4a52-b8c5-bf99fbad6ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_90149_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x127251bb0>"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view = urls_df[(urls_df[0].str.contains('\\?'))&(~urls_df[0].str.contains('event-offer'))]\n",
    "view.style.format({0: make_clickable})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7921434e-a375-4db2-8e52-91103943bcd0",
   "metadata": {},
   "source": [
    "The below code is useful for searching through the venue urls to see the different venues and to find different packages available at the venues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "6cc8e9c2-cef3-4bb8-9f11-d37ef86a8974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_3c529_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_3c529_level0_row0\" class=\"row_heading level0 row0\" >280</th>\n",
       "      <td id=\"T_3c529_row0_col0\" class=\"data row0 col0\" ><a href=\"https://www.tagvenue.com/rooms/london/3903/tanner-warehouse/tanner-warehouse-courtyard\">https://www.tagvenue.com/rooms/london/3903/tanner-warehouse/tanner-warehouse-courtyard</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c529_level0_row1\" class=\"row_heading level0 row1\" >2055</th>\n",
       "      <td id=\"T_3c529_row1_col0\" class=\"data row1 col0\" ><a href=\"https://www.tagvenue.com/rooms/london/3308/tanner-warehouse/industrial-wedding\">https://www.tagvenue.com/rooms/london/3308/tanner-warehouse/industrial-wedding</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c529_level0_row2\" class=\"row_heading level0 row2\" >3639</th>\n",
       "      <td id=\"T_3c529_row2_col0\" class=\"data row2 col0\" ><a href=\"https://www.tagvenue.com/rooms/london/321/tanner-warehouse/tanner-warehouse\">https://www.tagvenue.com/rooms/london/321/tanner-warehouse/tanner-warehouse</a></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x127242c40>"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Switch between space_urls_unique and space_urls_cleaned to \n",
    "# get with and without packages, and to find specific venues \n",
    "# or venues with packages e.g. search for '\\?event-offer=wedding' \n",
    "# to get venues with wedding packages\n",
    "urls_df = pd.DataFrame(space_urls_unique)\n",
    "view = urls_df[urls_df[0].str.contains('south')]\n",
    "view.style.format({0: make_clickable})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fe2e76-806b-4eec-9608-479292bb76b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from operator import itemgetter\n",
    "local_vars = list(locals().items())\n",
    "# Size gives us variable size in Bytes\n",
    "size = [[var,sys.getsizeof(obj)] for var, obj in local_vars]\n",
    "size = sorted(size, key=itemgetter(1), reverse = True)\n",
    "for var, size in size:\n",
    "    print(var,f\"-> {size/1000000:,} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7758c0ee-4168-4e7f-88e8-b51ac232b203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_space_urls()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "302549f4-6b70-4567-bae8-4ae69780eba5",
   "metadata": {},
   "source": [
    "error log as well...may want error log top include event type + page number etc. in case of failure...can easily do with the returns Fail command \n",
    "\n",
    "Also separate error log from tagvenue bad url "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "18982825-2b8d-452c-aa63-0cba63540742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 a\n",
      "2 a\n",
      "1 b\n",
      "2 b\n",
      "1 c\n",
      "2 c\n"
     ]
    }
   ],
   "source": [
    "for b in ['a','b','c']:\n",
    "    for a in range(1,2 + 1):\n",
    "        print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0b6d3059-a517-4d30-a184-5998f169b22e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page = 1\n",
    "search_url = create_search_url('Christmas-Dinner', page)\n",
    "load_page(search_url) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c663a1c-cf38-4da7-96f4-9cc157c6eeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccb1691-8e57-4428-8ecc-8332510cfcf8",
   "metadata": {},
   "source": [
    "# Collect Venue URLs from Search Results\n",
    "The Hire Space search results show 18 results on the page, where each result is a clickable picture of a venue. These venue's are never duplicated in the results, you only see the venue once in the results page even if the venue has multiple spaces available within it. To see more results, you must click the 'show more' button at the bottom of the page, which will reveal a further 18 results. This can be repeated until all results are showing.\n",
    "\n",
    "We will use Selenium to click 'show more' every 10 seconds (to minimise the impact on Hire Space's servers) until all venues returned by the search are made visible. \n",
    "\n",
    "We then scrape the url that each search result links to when you click on it and store these urls in a list. This list of urls forms the scope of our web scrape - we want to scrape data from every venue in this list and therefore from each of these urls. We call these the *venue urls* because they provide a unique link from the search results to the venue (since a venue never appears twice in the search results). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bb3a299-d5a4-44b7-849b-55176e92ea6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2729 venues\n"
     ]
    }
   ],
   "source": [
    "# Flag to indicate 'show more' button is clickable \n",
    "more_search_results = True\n",
    "# Loop that clicks on 'show more' until all search results are visible \n",
    "# (loops until you can no longer click on 'show more') \n",
    "while (more_search_results):\n",
    "    # Finds the html element of the 'show more' button\n",
    "    show_more = driver.find_element_by_xpath(\n",
    "        \"//button[@class='btn btn-default btn-large btn-block']\")\n",
    "    # Make chrome click the 'show more' button. If this causes an error, \n",
    "    # then end the while loop (because all results are now visible)\n",
    "    try:\n",
    "        # Wait 10 seconds to minimise server impact\n",
    "        time.sleep(10)\n",
    "        show_more.click()\n",
    "    except ElementNotInteractableException:  \n",
    "        # End while loop\n",
    "        more_search_results = False\n",
    "    \n",
    "# Finds and places the html element of each venue returned by the\n",
    "# search (each venue appears as a clickible picture in chrome) into a list   \n",
    "venues = driver.find_elements_by_xpath(\"//div[@class='searchresult']/a\")\n",
    "    \n",
    "# Extract url each search result html element links to\n",
    "venue_urls = [venue.get_attribute('href') for venue in venues]\n",
    "print('There are', len(venue_urls), 'venues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3125b7a1-3943-4c9b-a0be-b91fba859c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save venue url list to file as a csv\n",
    "with open('venue_url_list', 'w') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerow(venue_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d91cbfd-c6e6-432c-ba09-095118ed5213",
   "metadata": {},
   "source": [
    "### Venue Crawl Strategy\n",
    "Each venue has one or more spaces and each space has a dedicated webpage. We will scrape the full html code of every space's web page for all venues returned in the search. If *scrape_spaces* was set to False, we will only scrape a single space webpage from each venue. \n",
    "\n",
    "We scrape the full web page rather than scraping individual elements to avoid unexpected errors when scraping e.g. from an inconsistent html element. The web scraping process takes several hours to complete due to waiting several seconds between subsequent page loads. Once we have scraped the full set of web pages, we will extract the specific data we want. Any errors encountered due to inconsistent html or incompletely loaded pages etc. can be fixed quickly, without crashing the slow web scraping process.\n",
    "\n",
    "The venue urls from the search results page each link to a different venue. They will take you to the web page of a space within the venue. If the venue offers more than one space, a button will be visible called 'See All Spaces Here'. This links to a general overview page for the venue, showing clickable links to all spaces within the venue. \n",
    "\n",
    "The following process was used to scrape the space web pages from a single venue url: \n",
    "\n",
    "1. Load venue url from search results page -> takes you to web page for a space in the venue\n",
    "2. Save full page html code \n",
    "3. If 'See All Spaces Here' button exists\n",
    "   1. Load 'See All Spaces Here' url -> takes you to venue overview page  \n",
    "   2. Gather urls of each space within venue from venue overview page\n",
    "   2. load each space url in turn and scrape full page html\n",
    "\n",
    "If an error occurs when loading a webpage during this process, the scrape is aborted for that venue and the error is logged. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc70553f-a3ea-496c-b9cd-6cbf2e07c836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stores the url and full html for every space web page scraped. \n",
    "# Each element of raw data is a list [url, html]. \n",
    "raw_data = []\n",
    "\n",
    "def scrape_venue(venue_url):\n",
    "    \"\"\"Scrapes full html of all the spaces in the venue related to the \n",
    "    provided venue_url\"\"\"\n",
    "    \n",
    "    global error_flag\n",
    "    # Loads the venue url in chrome. This opens the web page for a space\n",
    "    # in the venue.  \n",
    "    load_page(venue_url)\n",
    "    # If there was a page load error, break out of function, dont scrape the venue\n",
    "    if (error_flag):\n",
    "        error_flag = False\n",
    "        return\n",
    "    # Saves html of the space web page currently showing in chrome \n",
    "    raw_data.append([venue_url, driver.page_source])\n",
    "    # Find html element for 'see all spaces here' button, returns a list\n",
    "    # of length 0 if the button is not available\n",
    "    see_all_spaces_here = driver.find_elements_by_xpath(\n",
    "        \"//a[@class='btn btn-default btn-block btn-lgText']\")\n",
    "    # If more spaces exist that need to be downloaded (if 'see all spaces \n",
    "    # here' button exists) and scrape_spaces is True, load the venue \n",
    "    # overview, which contains the other spaces to be scraped. \n",
    "    if (len(see_all_spaces_here) == 1) & (scrape_spaces):\n",
    "        venue_overview_url = see_all_spaces_here[0].get_attribute('href')\n",
    "        load_page(venue_overview_url, venue_url)\n",
    "        # If there was a page load error, break out of function, dont scrape\n",
    "        # the rest of venue\n",
    "        if (error_flag):\n",
    "            error_flag = False\n",
    "            return\n",
    "        # get html elements for all spaces in venue\n",
    "        all_spaces_for_venue = driver.find_elements_by_xpath(\n",
    "            \"//a[@class='btn']\")\n",
    "        # Extract url for every space in venue\n",
    "        all_space_urls_for_venue = [space.get_attribute('href') \n",
    "                                    for space in all_spaces_for_venue]\n",
    "        # Need to avoid re-scraping the space page loaded by venue_url. \n",
    "        # This is done by matching the space name in the venue_url with\n",
    "        # the space name in the urls taken from the venue overview page.\n",
    "        # Extract space name from venue_url\n",
    "        space_name_from_venue_url = venue_url.split(\"/\")[-2]\n",
    "        for space_url in all_space_urls_for_venue:\n",
    "            # Extract space name from url taken from overview page\n",
    "            space_name_from_overview_url = space_url.split(\"/\")[-2]\n",
    "            # Checks url doesnt link to same space as venue_url \n",
    "            if (space_name_from_overview_url != space_name_from_venue_url):\n",
    "                load_page(space_url, venue_url)\n",
    "                # If there was a page load error, break out of function,\n",
    "                # dont scrape the rest of venue\n",
    "                if (error_flag):\n",
    "                    error_flag = False\n",
    "                    return\n",
    "                # Saves html of the space web page currently showing in chrome\n",
    "                raw_data.append([space_url, driver.page_source])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231c9396-287f-4ac7-8d75-dafc0f29c296",
   "metadata": {},
   "source": [
    "### Scrape Venues\n",
    "The process described above was applied to every venue url taken from the search results page to scrape all the required venue data. This is a slow process due to the waiting several seconds between subsequent page loads, adding up to hours of waiting time.\n",
    "\n",
    "A periodic process is run during the scraping to provide progress updates and save the scraped data to file in case of any crashes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8ae1d92-6d22-405b-bd82-c80b55e90167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.0% Completed -> Scraped 354 out of 2729 venues\n",
      "Spent 0.5 hours scraping \n",
      "\n",
      "25.1% Completed -> Scraped 685 out of 2729 venues\n",
      "Spent 1.0 hours scraping \n",
      "\n",
      "37.4% Completed -> Scraped 1021 out of 2729 venues\n",
      "Spent 1.5 hours scraping \n",
      "\n",
      "49.0% Completed -> Scraped 1336 out of 2729 venues\n",
      "Spent 2.0 hours scraping \n",
      "\n",
      "56.0% Completed -> Scraped 1528 out of 2729 venues\n",
      "Spent 2.5 hours scraping \n",
      "\n",
      "67.5% Completed -> Scraped 1843 out of 2729 venues\n",
      "Spent 3.0 hours scraping \n",
      "\n",
      "78.9% Completed -> Scraped 2153 out of 2729 venues\n",
      "Spent 3.5 hours scraping \n",
      "\n",
      "88.5% Completed -> Scraped 2415 out of 2729 venues\n",
      "Spent 4.0 hours scraping \n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_venues = len(venue_urls)\n",
    "time_last_progress_update = time.time() \n",
    "scraping_time = 0\n",
    "\n",
    "# Loop through venue urls and scrape data from each venue. \n",
    "for venue_num, venue_url in enumerate(venue_urls):\n",
    "    scrape_venue(venue_url)\n",
    "    # Provide progress update and hard save scraped data every 30 mins \n",
    "    # in case of interruption to scrape\n",
    "    if time.time() - time_last_progress_update > backup_interval:\n",
    "        perc_complete = ((venue_num / total_venues))\n",
    "        scraping_time += 0.5\n",
    "        print(f\"{perc_complete:.1%} Completed -> Scraped {venue_num} out of {total_venues} venues\")\n",
    "        print(f\"Spent {scraping_time} hours scraping\", '\\n')   \n",
    "        time_last_progress_update = time.time()\n",
    "        # Save data scraped so far to csv\n",
    "        with open(\"backup_html_pages.csv\", \"w\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "325555ee-eeac-4c16-bab8-4b101c238371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all scraped data to csv\n",
    "with open(\"html_pages.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43854390-c4e8-4701-8b5d-87060e7c747f",
   "metadata": {},
   "source": [
    "### Scraping Error Log\n",
    "If the scraping of a venue was aborted due to a page load error, it is displayed below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "842152b3-f6a5-43ca-b4a5-14c38168827e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_04e3f_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >venue_url</th>\n",
       "      <th class=\"col_heading level0 col1\" >problem_url</th>\n",
       "      <th class=\"col_heading level0 col2\" >error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x10ac64fa0>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors = pd.DataFrame(load_page_error_log, columns = ['venue_url','problem_url','error'])\n",
    "# Function to make urls clickable in jupyter\n",
    "def make_clickable(val):\n",
    "    return '<a href=\"{}\">{}</a>'.format(val,val)\n",
    "\n",
    "errors.style.format({'venue_url': make_clickable,'problem_url': make_clickable})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905e84ca-1320-4e94-8e1e-7e8bde459197",
   "metadata": {},
   "source": [
    "### Load Saved HTML Data\n",
    "The below commented out code allows you to load the html_pages.csv file if you want to come back to it without scraping all the data again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "652b8caf-5889-4999-86bb-7833430a3e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv.field_size_limit(sys.maxsize)\n",
    "#with open('html_pages.csv', newline='') as f:\n",
    "#    reader = csv.reader(f)\n",
    "#    raw_data = list(reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dde0a7-b4d2-4b4a-aaa8-93b4091d42d8",
   "metadata": {},
   "source": [
    "### Extract Data From HTML\n",
    "We will use the lxml library to extract useful data from the html code of each space's website. The extraction code handles 2 specific issues detailed below: \n",
    "- Some spaces were no longer hosted by Hire Space and therefore had no data available. The code excludes these venues by checking whether the start of the header reads 'venue no longer hosted'. \n",
    "- Some web pages didn't load properly during scraping. They were missing data and trigger an error during extraction. The extraction code deals with them by re-loading the webpage and trying to extract data from the reloaded page's html.  \n",
    "Any other errors will be handled by aborting the extraction process and adding the error to an error log. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31b96961-3d0d-46db-95c6-74db34571d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error log for extraction errors\n",
    "extraction_errors = []\n",
    "\n",
    "def extract_data(website_data):\n",
    "    \"\"\"Returns extracted data from an element of raw_data. \n",
    "    \n",
    "    Each element of raw_data is in form [url, html]. This\n",
    "    function handles and logs errors during extraction and \n",
    "    sends the html to the extract_from_html function for extraction.\n",
    "    \n",
    "    Returns - Extracted data as a list or returns None if unable to \n",
    "    extract data\n",
    "    \"\"\"\n",
    "    \n",
    "    url = website_data[0]\n",
    "    # check if the venue is no longer hosted\n",
    "    if (check_page_not_hosted(website_data[1])):\n",
    "        # Log extraction error due to venue no longer hosted \n",
    "        extraction_errors.append([url, 'venue no longer hosted',\n",
    "                                  website_data[1]])\n",
    "        return None\n",
    "    try:\n",
    "        # Extract the data from the html code\n",
    "        return extract_from_html(url, website_data[1])\n",
    "    except:\n",
    "        # If initial data extraction fails, re-load the url in chrome\n",
    "        load_page(url)\n",
    "        if \n",
    "        try:\n",
    "            # Try to extract the data using the html code of the \n",
    "            # re-loaded page\n",
    "            return extract_from_html(url, driver.page_source)  \n",
    "        except Exception as e:\n",
    "            # Log error, including error message in log\n",
    "            extraction_errors.append([url, e, website_data[1]])\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afdaad98-7e01-4e69-b616-f964958b9ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHEN EXPANDING FOR RICH DATA - test and run this function before the 'extract data' \n",
    "# function\n",
    "\n",
    "def extract_from_html(url, website_html):\n",
    "    \"\"\"Extracts and returns data from website_html.\"\"\"\n",
    "    # Parse html with lxml library\n",
    "    tree = etree.HTML(website_html)\n",
    "    \n",
    "    # Extract venue and space name from h1 and h2 headers\n",
    "    venue_name = tree.xpath(\"//h1/text()\")[0].lower()\n",
    "    space_name = tree.xpath(\"//h2/text()\")[0]\n",
    "    # Remove trailing ' in' text\n",
    "    space_name = space_name[:-3].lower()\n",
    "\n",
    "    # Find html element for map of venue location\n",
    "    space_map = tree.xpath(\"//div[@id='map_canvas']\")[0]\n",
    "    # Extract 'style' attribute from element, this contains the \n",
    "    # longitude and latitude in a url\n",
    "    space_map_style = space_map.get('style')\n",
    "    # Extract the longitude and latitude from url, they are\n",
    "    # between the first '=' and first '&' in the url \n",
    "    style_start_of_long_lat = space_map_style.find('=')\n",
    "    style_end_of_long_lat = space_map_style.find('&')\n",
    "    long_lat = space_map_style[style_start_of_long_lat : style_end_of_long_lat]\n",
    "    # longitude and latitude in form of string '=latitude%2C-longitude'. \n",
    "    # Split longitude and latitude into list, using the % as delimiter\n",
    "    # then clean up by removing excess text characters \n",
    "    long_lat = long_lat.split('%')\n",
    "    long_lat[0] = long_lat[0].replace('=','')\n",
    "    long_lat[1] = long_lat[1].replace('2C','')\n",
    "    # Convert to float\n",
    "    long_lat = [float(item) for item in long_lat]\n",
    "    \n",
    "    # Find the html elements of the address (5 seperate items in a list)\n",
    "    address = tree.xpath(\"//div[@class='address-overlay']/ul//li\")\n",
    "    # Extracts text from each html element of the address. If no \n",
    "    # text exist, it returns 'None'\n",
    "    address = [line.text for line in address]\n",
    "    # put address in lower case, ignoring entries which say 'None' \n",
    "    address = [item.lower() if (item != None) else item for item in address]\n",
    "    # remove space in postcode\n",
    "    address[4] = address[4].replace(' ','')\n",
    "\n",
    "    return [url, venue_name, space_name, long_lat[0], long_lat[1], \n",
    "                address[0], address[1], address[2], address[3], address[4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26d6f061-03b8-48ba-ba64-669d055e1f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_page_not_hosted(website_html): \n",
    "    \"\"\"Check if venue no longer hosted, return result (True or False)\"\"\"\n",
    "    \n",
    "    # Parse html with lxml library\n",
    "    tree = etree.HTML(website_html)\n",
    "    # Find text of h1 heading\n",
    "    heading = tree.xpath(\"//h1/text()\")\n",
    "    # Ensure heading was returned (will return list length 0 if \n",
    "    # no heading found)\n",
    "    if (len(heading) > 0):\n",
    "        # Return true if heading matches expected heading \n",
    "        # for no longer hosted page\n",
    "        return (heading[0][:34] == 'Hire Space does not currently list')\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb2c4381-7bd2-495c-bb24-8f41f398c80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2693 spaces were successfully scraped\n"
     ]
    }
   ],
   "source": [
    "# Extract data from the space webpages that were scraped\n",
    "data = [extract_data(website_data) for website_data in raw_data]\n",
    "# Remove None entries in data (These are from the erroneous \n",
    "# web pages that couldn't be scraped)\n",
    "data = [item for item in data if (item != None)]\n",
    "print(f\"{len(data)} spaces were successfully scraped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e94841de-ef45-4d48-9e95-092ce17133b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 spaces could not be scraped due to errors while extracting the data\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(extraction_errors)} spaces could not be scraped due to errors while extracting the data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a2f186-4b7d-404b-9911-26cab49c56d8",
   "metadata": {},
   "source": [
    "### Extraction Error Log\n",
    "If extraction failed on a webpage, display error below along with error message. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2fdc9c4-1e8d-4922-8624-39523e9ae584",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_clickable' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3z/dqb7xjp13pg6n8dk8d2gscy40000gn/T/ipykernel_13865/3816307585.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# View error log, making urls clickable and excluding the html code in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# the error log from view\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0merrors_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmake_clickable\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'make_clickable' is not defined"
     ]
    }
   ],
   "source": [
    "errors_2 = pd.DataFrame(extraction_errors, columns = ['url','error','html'])\n",
    "\n",
    "# View error log, making urls clickable and excluding the html code in\n",
    "# the error log from view\n",
    "errors_2[['url','error']].style.format({'url': make_clickable})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5f1e2e-6195-45d8-a849-b009d8f71e68",
   "metadata": {},
   "source": [
    "### Summarise and Save Data\n",
    "Below we convert the extracted data to a dataframe, show first 10 rows and summary statistics. We then save the data to file as a csv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b183875-12c2-467d-b288-757709a42e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>venue_name</th>\n",
       "      <th>space_name</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>address_line_1</th>\n",
       "      <th>address_line_2</th>\n",
       "      <th>address_line_3</th>\n",
       "      <th>address_line_4</th>\n",
       "      <th>address_line_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://hirespace.com/Spaces/London/187488/Ano...</td>\n",
       "      <td>anomalous space</td>\n",
       "      <td>georgian townhouse</td>\n",
       "      <td>51.532138</td>\n",
       "      <td>-0.108149</td>\n",
       "      <td>anomalous space</td>\n",
       "      <td>36-38 pentonville road</td>\n",
       "      <td>angel, islington</td>\n",
       "      <td>london</td>\n",
       "      <td>n19hf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://hirespace.com/Spaces/London/107767/The...</td>\n",
       "      <td>the zetter townhouse, clerkenwell</td>\n",
       "      <td>the games room</td>\n",
       "      <td>51.523170</td>\n",
       "      <td>-0.103552</td>\n",
       "      <td>49-50 st john's square</td>\n",
       "      <td>None</td>\n",
       "      <td>clerkenwell, farringdon</td>\n",
       "      <td>None</td>\n",
       "      <td>ec1v4jj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://hirespace.com/Spaces/London/139311/The...</td>\n",
       "      <td>the postal museum</td>\n",
       "      <td>the courtyard</td>\n",
       "      <td>51.524727</td>\n",
       "      <td>-0.113533</td>\n",
       "      <td>15-20 phoenix place</td>\n",
       "      <td>phoenix place</td>\n",
       "      <td>farringdon</td>\n",
       "      <td>greater london</td>\n",
       "      <td>wc1x0da</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://hirespace.com/Spaces/London/131840/The...</td>\n",
       "      <td>the hac (honourable artillery company)</td>\n",
       "      <td>prince consort rooms</td>\n",
       "      <td>51.523119</td>\n",
       "      <td>-0.087275</td>\n",
       "      <td>armoury house</td>\n",
       "      <td>city road</td>\n",
       "      <td>old street</td>\n",
       "      <td>london</td>\n",
       "      <td>ec1y2bq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://hirespace.com/Spaces/London/189080/The...</td>\n",
       "      <td>the phoenix london</td>\n",
       "      <td>full venue hire</td>\n",
       "      <td>51.506317</td>\n",
       "      <td>-0.223909</td>\n",
       "      <td>the phoenix london, westfield london shopping ...</td>\n",
       "      <td>None</td>\n",
       "      <td>shepherds bush, west london, london</td>\n",
       "      <td>None</td>\n",
       "      <td>w127ga</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://hirespace.com/Spaces/London/187488/Ano...   \n",
       "1  https://hirespace.com/Spaces/London/107767/The...   \n",
       "2  https://hirespace.com/Spaces/London/139311/The...   \n",
       "3  https://hirespace.com/Spaces/London/131840/The...   \n",
       "4  https://hirespace.com/Spaces/London/189080/The...   \n",
       "\n",
       "                               venue_name            space_name  longitude  \\\n",
       "0                         anomalous space    georgian townhouse  51.532138   \n",
       "1       the zetter townhouse, clerkenwell        the games room  51.523170   \n",
       "2                       the postal museum         the courtyard  51.524727   \n",
       "3  the hac (honourable artillery company)  prince consort rooms  51.523119   \n",
       "4                      the phoenix london       full venue hire  51.506317   \n",
       "\n",
       "   latitude                                     address_line_1  \\\n",
       "0 -0.108149                                    anomalous space   \n",
       "1 -0.103552                             49-50 st john's square   \n",
       "2 -0.113533                                15-20 phoenix place   \n",
       "3 -0.087275                                      armoury house   \n",
       "4 -0.223909  the phoenix london, westfield london shopping ...   \n",
       "\n",
       "           address_line_2                       address_line_3  \\\n",
       "0  36-38 pentonville road                     angel, islington   \n",
       "1                    None              clerkenwell, farringdon   \n",
       "2           phoenix place                           farringdon   \n",
       "3               city road                           old street   \n",
       "4                    None  shepherds bush, west london, london   \n",
       "\n",
       "   address_line_4 address_line_5  \n",
       "0          london          n19hf  \n",
       "1            None        ec1v4jj  \n",
       "2  greater london        wc1x0da  \n",
       "3          london        ec1y2bq  \n",
       "4            None         w127ga  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data, columns = ['url','venue_name','space_name', 'longitude','latitude', 'address_line_1',\n",
    "                                   'address_line_2','address_line_3','address_line_4','address_line_5'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc2e673e-7166-4c81-8f11-a2df2bd2ea42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>venue_name</th>\n",
       "      <th>space_name</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>address_line_1</th>\n",
       "      <th>address_line_2</th>\n",
       "      <th>address_line_3</th>\n",
       "      <th>address_line_4</th>\n",
       "      <th>address_line_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2693</td>\n",
       "      <td>2693</td>\n",
       "      <td>2693</td>\n",
       "      <td>2693.000000</td>\n",
       "      <td>2693.000000</td>\n",
       "      <td>2693</td>\n",
       "      <td>936</td>\n",
       "      <td>2453</td>\n",
       "      <td>1529</td>\n",
       "      <td>2693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2693</td>\n",
       "      <td>2688</td>\n",
       "      <td>1446</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2610</td>\n",
       "      <td>810</td>\n",
       "      <td>638</td>\n",
       "      <td>113</td>\n",
       "      <td>2345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>https://hirespace.com/Spaces/London/187488/Ano...</td>\n",
       "      <td>bianca road brew co</td>\n",
       "      <td>whole venue</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unit 3, greenwich business park, 53 norman rd,...</td>\n",
       "      <td>london</td>\n",
       "      <td>shoreditch</td>\n",
       "      <td>london</td>\n",
       "      <td>se100dx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>543</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>57</td>\n",
       "      <td>110</td>\n",
       "      <td>1066</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.513439</td>\n",
       "      <td>-0.121308</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.036504</td>\n",
       "      <td>0.073681</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.326626</td>\n",
       "      <td>-0.446500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.501920</td>\n",
       "      <td>-0.149493</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.513676</td>\n",
       "      <td>-0.119519</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.525684</td>\n",
       "      <td>-0.080269</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.729776</td>\n",
       "      <td>0.219075</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      url  \\\n",
       "count                                                2693   \n",
       "unique                                               2693   \n",
       "top     https://hirespace.com/Spaces/London/187488/Ano...   \n",
       "freq                                                    1   \n",
       "mean                                                  NaN   \n",
       "std                                                   NaN   \n",
       "min                                                   NaN   \n",
       "25%                                                   NaN   \n",
       "50%                                                   NaN   \n",
       "75%                                                   NaN   \n",
       "max                                                   NaN   \n",
       "\n",
       "                 venue_name   space_name    longitude     latitude  \\\n",
       "count                  2693         2693  2693.000000  2693.000000   \n",
       "unique                 2688         1446          NaN          NaN   \n",
       "top     bianca road brew co  whole venue          NaN          NaN   \n",
       "freq                      2          543          NaN          NaN   \n",
       "mean                    NaN          NaN    51.513439    -0.121308   \n",
       "std                     NaN          NaN     0.036504     0.073681   \n",
       "min                     NaN          NaN    51.326626    -0.446500   \n",
       "25%                     NaN          NaN    51.501920    -0.149493   \n",
       "50%                     NaN          NaN    51.513676    -0.119519   \n",
       "75%                     NaN          NaN    51.525684    -0.080269   \n",
       "max                     NaN          NaN    51.729776     0.219075   \n",
       "\n",
       "                                           address_line_1 address_line_2  \\\n",
       "count                                                2693            936   \n",
       "unique                                               2610            810   \n",
       "top     unit 3, greenwich business park, 53 norman rd,...         london   \n",
       "freq                                                    6             57   \n",
       "mean                                                  NaN            NaN   \n",
       "std                                                   NaN            NaN   \n",
       "min                                                   NaN            NaN   \n",
       "25%                                                   NaN            NaN   \n",
       "50%                                                   NaN            NaN   \n",
       "75%                                                   NaN            NaN   \n",
       "max                                                   NaN            NaN   \n",
       "\n",
       "       address_line_3 address_line_4 address_line_5  \n",
       "count            2453           1529           2693  \n",
       "unique            638            113           2345  \n",
       "top        shoreditch         london        se100dx  \n",
       "freq              110           1066              8  \n",
       "mean              NaN            NaN            NaN  \n",
       "std               NaN            NaN            NaN  \n",
       "min               NaN            NaN            NaN  \n",
       "25%               NaN            NaN            NaN  \n",
       "50%               NaN            NaN            NaN  \n",
       "75%               NaN            NaN            NaN  \n",
       "max               NaN            NaN            NaN  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f73970b-2ea7-40d2-9595-fc771b48fa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('hire_space_venue_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d8b172a-1365-408b-80aa-68f47fe3f2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close chrome page\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c49253-d65a-4032-8564-6e703da3faae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from operator import itemgetter\n",
    "local_vars = list(locals().items())\n",
    "# Size gives us variable size in Bytes\n",
    "size = [[var,sys.getsizeof(obj)] for var, obj in local_vars]\n",
    "size = sorted(size, key=itemgetter(1), reverse = True)\n",
    "for var, size in size:\n",
    "    print(var,f\"-> {size/1000000:,} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b5fb90-4c35-48c1-bf16-b9991dfbf6f6",
   "metadata": {},
   "source": [
    "### Investigating Extraction Errors\n",
    "Below provides examples of using the extraction_errors list to re-create the errors during extraction to aid with debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f9154001-fa72-40cc-9ad3-2cdb505b46aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'venue no longer hosted'\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracts error message from first error \n",
    "Error_number = 0\n",
    "repr(extraction_errors[Error_number][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eef63808-a451-4aef-aef8-8a7354007407",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3z/dqb7xjp13pg6n8dk8d2gscy40000gn/T/ipykernel_12982/2120079944.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Runs extraction on erroneous html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mError_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mextract_from_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextraction_errors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mError_number\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/3z/dqb7xjp13pg6n8dk8d2gscy40000gn/T/ipykernel_12982/980712329.py\u001b[0m in \u001b[0;36mextract_from_html\u001b[0;34m(url, website_html)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Find html element for map of venue location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mspace_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"//div[@id='map_canvas']\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Extract 'style' attribute from element, this contains the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# longitude and latitude in a url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Runs extraction on erroneous html, returning full original error\n",
    "Error_number = 0\n",
    "extract_from_html('url', extraction_errors[Error_number][2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Venue_scrape",
   "language": "python",
   "name": "venue_scrape"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
