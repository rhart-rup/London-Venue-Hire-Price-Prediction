{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb83bebd-c803-4a0d-be3c-044de1fecd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import chromedriver_autoinstaller\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException, NoSuchElementException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import lxml.html\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99aa074-802f-4721-ad3a-128e748530e6",
   "metadata": {},
   "source": [
    "![alt text](images/tag_venue_home_page.png)\n",
    "# Tagvenue Venue Web Scrape\n",
    "### Introduction \n",
    "\n",
    "The [Tagvenue](https://www.tagvenue.com/) website is basically an Air BnB for finding and booking venues for an event. The website hosts thousands of venues in the UK that can be booked for events such as weddings, work drinks, birthdays etc. Each venue has one or more **spaces** available to be booked. A **space** is basically a room or area within the venue. Some venues have a single space, often the whole venue, whilst others offer a selection of rooms, each offered as a separate space. Each Space has its own webpage on Tagvenue. This webpage contains all the data needed to choose which space to book for your event. Example data includes price, location, size, capacity, features, licensing etc. This notebook will scrape the data from all spaces on the [Tagvenue](https://www.tagvenue.com/) website that are located in **London**. At the time of writing this amounts to **~4400** spaces. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944bcae5-183f-46df-9b4b-d81ec9206ade",
   "metadata": {},
   "source": [
    "### Key Variables\n",
    "The following key variables define and tweak the specifics of the web scrape: \n",
    "\n",
    "- **progress_report_interval** - Periodic progress reports (% completed) are printed during scraping. This variable defines in seconds how often the report is output. \n",
    "- **connection_error_retry_time** - This defines how long in seconds the program will wait before trying to re-load a webpage when it fails to load due to a connection error. \n",
    "- **headless_mode** - Set to *True* if you want chrome to be launched in headless mode i.e. not visible. Set to *False* if you wish chrome to be visible while scraping.  \n",
    "- **longitude_min**, **longitude_max**, **latitude_min** and **latitude_max** - Defines the area that will be searched for venues. The intersection of the four longitude / latitude lines defines a square area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98af76c8-8b80-4250-b9e5-cf3050f70bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_report_interval = 1800 #1800 for normal run, 300 for test\n",
    "connection_error_retry_time = 300  # 300 for normal run, 30 for test\n",
    "# Set True to have chrome open in headless mode \n",
    "headless_mode = False \n",
    "# longitude and latidue max and min define four lines, the intersection \n",
    "# of these lines defines a square area used for the venue search\n",
    "# Normal run values, comment out when not wanted \n",
    "latitude_min = 51.326626 \n",
    "latitude_max = 51.7297765\n",
    "longitude_min = -0.446500003\n",
    "longitude_max = 0.2190751\n",
    "# Test Values, comment out when not wanted \n",
    "#longitude_min = -0.100501\n",
    "#longitude_max = -0.059614\n",
    "#latitude_min = 51.494423\n",
    "#latitude_max = 51.50697"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23126e8-c587-4642-924c-5df6d2f4d03a",
   "metadata": {},
   "source": [
    "### Initiate Web Scraper\n",
    "We will use Selenium and Chromedriver / Chrome to crawl the Hire Space website and scrape data. An initial check is performed by *chromedriver_autoinstaller()* to ensure chromedriver is up to data. If it is not, then the latest version is downloaded. Selenium then initiates an instance of chrome that it can control. This instance will either be visible or invisible (headless mode) depending on the *headless_mode* variable.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "8e270f6a-24e0-4cf1-87dc-25a668bb7f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the current version of chromedriver exists\n",
    "# and if it doesn't exist, download it automatically,\n",
    "# then add chromedriver to path\n",
    "chromedriver_autoinstaller.install()\n",
    "# If headless_mode was True, open chrome in headless mode, \n",
    "# otherwise open a visible chrome browser\n",
    "if (headless_mode):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "else:\n",
    "    # Initialise chromedriver\n",
    "    driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ac69ac-b1fe-4712-8c38-8393943df19a",
   "metadata": {},
   "source": [
    "### Define Page Load Function\n",
    "We will frequently load new webpages with Selenium. We want to wait a certain amount of time between successive page loads to minimise our impact on the server and avoid being detected as a bot. We also want to detect any connection errors that might occur during the loading of a page for example due to a wifi issue. \n",
    "\n",
    "For this purpose, we created the *load_page* function. This function basically takes a url and loads it into chrome. It then pauses the program for twice the time it took for the page to load (with random variation to look less like a bot). In this way, the strain we put on the server will dynamically change. The slower the server becomes, the more time we will wait between successive page loads and vice versa if the server speeds up. \n",
    "\n",
    "The function also Handles *timeout* (page took longer than 30 seconds to load) and *connection* errors (couldn't connect to internet). In either case, the programme will wait some time then try to reload the page. If it still fails, the error is logged.  \n",
    "\n",
    "The function returns *True* if there were no errors loading the page and returns *False* when there were errors. It can be placed in an *if* statement so that the page will only be processed if the page loaded successfully.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6493ded9-c781-4c33-bf30-d89d7e88296a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set chromedriver timeout error to trigger if page takes more \n",
    "# than 30 seconds to load\n",
    "driver.set_page_load_timeout(30)\n",
    "# Initialise scraping error log\n",
    "scraping_error_log = []\n",
    "# Note - the below function returns True when no errors occur \n",
    "# during page load and is designed to be put within an 'if' \n",
    "# i.e. if(load_page(url)): to only do the steps in the 'if' \n",
    "# when the page load doesn't have errors\n",
    "def load_page(url):\n",
    "    \"\"\"Load provided url in chrome then sleep for interval of time. \n",
    "    \n",
    "    Handles and logs timout and connection errors. Calculates \n",
    "    the time to wait by multiplying the time it took the page to \n",
    "    load by 2, then adding some random offset.\n",
    "    \n",
    "    Returns -- True if page load was successful, returns False if there \n",
    "    was an error\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Loads url in chrome and calculates the time it took to load page\n",
    "        time_of_request = time.time()\n",
    "        driver.get(url)\n",
    "        page_load_time = time.time() - time_of_request\n",
    "        # Calculate time required to wait before next url is \n",
    "        # loaded\n",
    "        wait_time_till_next_request = wait_time_calculation(page_load_time)\n",
    "        time.sleep(wait_time_till_next_request)\n",
    "        # Returns True to indicate page load had no errors\n",
    "        return True\n",
    "    # Execution pauses if timeout or connection issue occurs \n",
    "    except (TimeoutException, WebDriverException) as e:\n",
    "        time.sleep(connection_error_retry_time)  \n",
    "        try:\n",
    "            # Loads url in chrome and calculates the time it took to load page\n",
    "            time_of_request = time.time()\n",
    "            driver.get(url)\n",
    "            page_load_time = time.time() - time_of_request\n",
    "            # Calculate time required to wait before next url is \n",
    "            # loaded (next time load_page is called)\n",
    "            wait_time_till_next_request = wait_time_calculation(page_load_time)\n",
    "            time.sleep(wait_time_till_next_request)\n",
    "        except TimeoutException:\n",
    "            scraping_error_log.append([url, \n",
    "                              'page failed to load, web page timed out'])\n",
    "            # Returns False to indicate page load had an error\n",
    "            return False\n",
    "        except WebDriverException:\n",
    "            scraping_error_log.append([url, \n",
    "                              'page failed to load, no internet connection'])\n",
    "            # Returns False to indicate page load had an error\n",
    "            return False\n",
    "            \n",
    "def wait_time_calculation(page_load_time):\n",
    "    \"\"\"Returns time required to wait before loading next url\n",
    "    \n",
    "    The wait time is 2 times the page_load_time, with \n",
    "    random variation\"\"\"\n",
    "    \n",
    "    average_wait_time = 2 * page_load_time\n",
    "    upper_wait_time = 1.33333 * average_wait_time\n",
    "    lower_wait_time = 0.77777 * average_wait_time\n",
    "    return random.uniform(lower_wait_time, upper_wait_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc1d286-fa8e-430a-9759-f98bddbdc572",
   "metadata": {},
   "source": [
    "### Creating Search URL\n",
    "![alt text](images/tag_venue_search_bar.png)\n",
    "We will use Tagvenue's [search page](https://www.tagvenue.com/) to find all venues located in London. The Tagvenue search requires an 'event type' to be chosen for the search. There are around **190** different 'event types' available to choose from. To find all venue's hosted by the website, we will have to repeat the search for all 190 available 'event types'. When a user clicks on the 'event type' field on the search page, a list of options is shown for them to choose from. Below we scrape the 'event type' options provided to the user.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea6be51-ba54-4d3d-a8c8-166db834b031",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagvenue_search_page_url = 'https://www.tagvenue.com/'\n",
    "if (load_page(tagvenue_search_page_url)):\n",
    "    # Find event type html input element\n",
    "    form_event_type_input = driver.find_element_by_xpath(\n",
    "        \"//input[@name='room_tag_autocomplete']\")\n",
    "    # Click on event type html input element - this loads the 'event \n",
    "    # type' html elements that contain the event type options into the\n",
    "    # webpage html\n",
    "    form_event_type_input.click()\n",
    "    # Find event type html elements \n",
    "    form_event_types_elements = driver.find_elements_by_xpath(\n",
    "        \"//div[@class='autocomplete-suggestions']//div\")\n",
    "    # Extract text from event types html elements \n",
    "    form_event_types = [element.get_attribute('innerHTML')\n",
    "                        for element in form_event_types_elements]\n",
    "    # replace spaces with '-', to make the event type conform to the \n",
    "    # url format used by Tagvenue - remove, from old approach \n",
    "    #event_types = [item.replace(' ','-') for item in event_types]\n",
    "else: raise Exception('page load error - cannot find event types')\n",
    "\n",
    "print(f\"There are {len(form_event_types)} event types on Tagvenue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109f9725-05a7-4efe-a86c-6a2bca059514",
   "metadata": {},
   "source": [
    "After you select an 'event type' and click search, a search url is created and loaded into chrome. You are then taken to the search results page. The 'event type' forms a sub-directory of the search url. An example search url is shown below, where the 'event type' chosen was '18th Birthday Party'.  \n",
    "\n",
    "https://www.tagvenue.com/uk/search/18th-birthday-party?location_id=6&people=&neighbourhood=London\n",
    "\n",
    "The text of the 'event type' options provided to the user when completing the form is not consistent with the text used in the url e.g. the event type *'Academic'* becomes *'academic-venues'* in the search url. As such, we will use selenium to perform a search using each 'event-type' option and will extract the text of the 'event-type' from the search url. This will enable us to create our own custom urls with custom longitude and latitude values later, rather than having to rely on the functionality of the search page.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5364d0ee-612e-462e-9c55-45fba282cb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_url_event_type(event_type):\n",
    "    \"\"\"Returns the event type text used in the search url for the\n",
    "    given event type\"\"\"\n",
    "    \n",
    "    load_page(tagvenue_search_page_url)\n",
    "    # Find event type input html element\n",
    "    event_type_input = driver.find_element_by_xpath(\n",
    "        \"//input[@name='room_tag_autocomplete']\")\n",
    "    # Input the event type\n",
    "    event_type_input.send_keys(event_type)\n",
    "    # Press enter\n",
    "    event_type_input.send_keys(Keys.ENTER)\n",
    "    # Find search button html element\n",
    "    search_button_element = driver.find_element_by_xpath(\n",
    "        \"//button[@class='c-button-cta c-button-cta--big js-hero-search']\")\n",
    "    # Click search button tom perform search\n",
    "    search_button_element.click()\n",
    "    try:\n",
    "        # Wait until page finishes loading following the click \n",
    "        WebDriverWait(driver, 10).until(\n",
    "            lambda d: d.execute_script('return document.readyState') == 'complete'\n",
    "        )\n",
    "    # If page fails to finish loading after 10 seconds, log error \n",
    "    except TimeoutException: \n",
    "        scraping_error_log.append([event_type, 'failed to load search results'])\n",
    "    # Extract search url used\n",
    "    search_url = driver.current_url\n",
    "    # Extract event type text from search url. Event type sits between\n",
    "    # the last '/' and first '?' in the search url.\n",
    "    end_of_event_type = search_url.find('?')\n",
    "    start_of_event_type = search_url.rfind('/', 0, end_of_event_type) + 1\n",
    "    return search_url[start_of_event_type : end_of_event_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88f185c-4333-4d17-a72d-fb7b07b37e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore, left in for testing / debugging purposes\n",
    "#form_event_types = form_event_types[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d0b237-089f-4c86-9c56-5049b69f96c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_event_types = []\n",
    "# Perform search on Tagvenue using each of the available event types\n",
    "# and extract the text used in the search url to denote the event type\n",
    "for form_event_type in form_event_types: \n",
    "    url_event_type = find_url_event_type(form_event_type)\n",
    "    url_event_types.append(url_event_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00635eed-106f-419b-83ab-5d2cb0e6ae81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Found {len(url_event_types)} search url event types\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed724a23-6f10-4a79-bc8a-85bc303c9473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first 10 event types\n",
    "url_event_types[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3319ba4e-a704-4056-abaf-e7a837f14a79",
   "metadata": {},
   "source": [
    "We can build a custom search url from an event type and latitude and longitude maximum and minimum values. This url will return search results showing all spaces within the defined latitude and longitude range that are suitable for the event type chosen. We chose to use a custom url so that we had full control over the latitude and longitude ranges. If we had used the Tagvenue search page to generate a search url, we would have to rely on Tagvenue's definition of the area of London.   \n",
    "\n",
    "An example custom search url is shown below: \n",
    "\n",
    "https://www.tagvenue.com/uk/search/18th-birthday-party?longitude_from=-0.270&longitude_to=0.069&latitude_from=51.31&latitude_to=51.69&page=1\n",
    "\n",
    "We also include the page number in the url. The search results display 36 results per page. The page number defines which page of the results to show. We included this in our custom url because the Tagvenue website alters the url in Chrome after it shows you the results. This alteration changes the latitude and longitude values. As such, if you navigate to another page by selecting a page using the page navigation links at the bottom of the results, it will display spaces from the new longitude and latitude range, and not from the range in the original search url. As a result, when we want to navigate to a new page of the search results, we create a new search result url with an updated page number. This keeps the latitude and longitude range constant as we navigate through results pages.   \n",
    "\n",
    "Below, we define the function to create a custom search url from an event type and page number (note that the latitude and longitude ranges are defined in the Key Variables section above.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16425f04-c132-41aa-86c9-4bc3bc0b4290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tagvenues changes the longitude and latitude values of the url in \n",
    "# Chrome after you load the url, so you need to recreate the whole url\n",
    "# whenever you change page or event type to keep the results within \n",
    "# the desired longitude and latitude range. \n",
    "def create_search_url(event_type, page):\n",
    "    \"\"\"Build and return search url string\"\"\"\n",
    "    return f\"\"\"https://www.tagvenue.com/uk/search/{event_type}?\n",
    "           longitude_from={longitude_min}&longitude_to={longitude_max}\n",
    "           &latitude_from={latitude_min}&latitude_to={latitude_max}&page={page}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df6e525-79dd-43b1-b462-8206e2e09f53",
   "metadata": {},
   "source": [
    "### Scraping Event Types Error Log\n",
    "If an error occurred whilst scraping the event types it is displayed below.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fd3347-95e8-486b-9d2c-9e56c3b107ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_errors = pd.DataFrame(scraping_error_log, columns = ['event_type','error'])\n",
    "# Function to make urls clickable in jupyter\n",
    "def make_clickable(val):\n",
    "    return '<a href=\"{}\">{}</a>'.format(val,val)\n",
    "\n",
    "scrape_errors.style.format({'url': make_clickable})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb46652c-e782-4c30-b678-98bc06a56db2",
   "metadata": {},
   "source": [
    "### Collect Space URLs From Search Results\n",
    "\n",
    "Using custom search urls, we can search for space's within our defined latitude and longitude ranges. The url will return a search results page as shown in the image below:  \n",
    "\n",
    "![alt text](images/tag_venue_search_results.png)\n",
    "\n",
    "Each space returned by the search is shown as a clickable picture. Clicking on the picture will take you to the web page of that space. We want to navigate through every page of search results and to gather the urls of every space web page shown. The below functions will help us achieve this, the first returns the total number of pages of results and the second scrapes all the space web page urls showing on the current page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f2237c-0776-4187-b96b-87aec2a22188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_total_results_pages():\n",
    "    \"\"\"Returns the number of pages of search results showing in Chrome\"\"\"\n",
    "    # Find pagination html elements - these create the clickable page\n",
    "    # numbers and arrows at bottom of search results page to naviagte \n",
    "    # through search results pages \n",
    "    pagination_elements = driver.find_elements_by_xpath(\n",
    "        \"//div[@class='results-pagination results-pagination--center']/ul/li/a\")\n",
    "    # Convert pagination elements to text values in a list e.g. it may \n",
    "    # look like [<< 1 2 3 ... 17 >>] \n",
    "    pagination = [element.get_attribute('innerHTML') \n",
    "                  for element in pagination_elements]\n",
    "    # If list is not empty i.e. len > 0 then return second last \n",
    "    # element - this is the total number of pages\n",
    "    if (len(pagination) > 1):\n",
    "        return int(pagination[-2])\n",
    "    # If list empty, then there is only one page, return 1\n",
    "    else: return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe282fa3-74ac-4818-aa66-8fcb875553df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block is just for creating a search url for debugging purposes \n",
    "#event_type = 'pop-up-event'\n",
    "#event_type = 'corporate-event'\n",
    "#page = 1\n",
    "#search_url = create_search_url('kids-partybus', page)\n",
    "#load_page(search_url) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297701bd-c646-4232-9c0b-7029a690f445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_space_urls():\n",
    "    \"\"\"Returns the url of every space webpage result showing in Chrome.\n",
    "    \n",
    "    Tagvenue will show a maximum of 36 results per page. This function\n",
    "    will return the urls for these 36 (or fewer) results currently visible. \n",
    "    If no venues are returned by the search, the function checks that this \n",
    "    is due to the search not finding anything and not because an erroneous \n",
    "    page was loaded\"\"\"\n",
    "    # Find the html elements that store the url of each space search result \n",
    "    # (this is the url that is opened when you click on a search result). \n",
    "    search_result_url_elements = driver.find_elements_by_xpath(\n",
    "        \"//div[@class='v-search-results-items']/div/a\")\n",
    "    # Checks whether no search result urls were found i.e. list was empty  \n",
    "    if(len(search_result_url_elements) == 0):\n",
    "        # When a search result returns no results, Tagvenue displays a h3 \n",
    "        # html text message that says 'sorry, we couldnt find any venues \n",
    "        # matching your criteria.'\n",
    "        try:\n",
    "            # Find h3 html element\n",
    "            no_search_results_message_element = (driver\n",
    "                                                 .find_element_by_xpath(\"//h3\")\n",
    "                                            )\n",
    "            # Extract text from h3 element \n",
    "            no_search_results_message = (no_search_results_message_element\n",
    "                                         .get_attribute('innerHTML')\n",
    "                                    )\n",
    "            # remove whitespace and apostrophies and lower case of text message \n",
    "            no_search_results_message = (no_search_results_message\n",
    "                                         .replace(\"'\", \"\").lower().strip()\n",
    "                                    )\n",
    "            expected_message = (\n",
    "                'sorry, we couldnt find any venues matching your criteria.')\n",
    "            # If message doesn't match the expected search result message \n",
    "            # then log possible error  \n",
    "            if(no_search_results_message != expected_message):\n",
    "                scraping_error_log.append([search_url, 'search url failed'])\n",
    "        # If no h3 element exists i.e. no search error message showing, \n",
    "        # log possible error. \n",
    "        except NoSuchElementException: \n",
    "            scraping_error_log.append([search_url, 'search url failed'])\n",
    "    # Extract urls from html elements and return them. IF no urls found, \n",
    "    # this will return an empty list. \n",
    "    return [element.get_attribute('href') for element in search_result_url_elements]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a819b11-bda1-4ee2-9dc6-b81eafd969b2",
   "metadata": {},
   "source": [
    "The search results page will only show spaces which are suitable for the event type in the search url. As such, we will need to repeat the search for all ~190 event types to ensure that we find every space hosted on the website. The spaces will be duplicated in different search results i.e. sometimes the same space will appear for 'corporate event' and '18th Birthday party'. As such, we will need to remove duplicates at the end. We could have used a *set()* to hold the list of space urls, which would negate the need to remove duplicates at the end. However, it was determined that it was useful when reporting the progress of the scrape, to be able to count the total number of urls scraped.\n",
    "\n",
    "Below, we perform a separate search for every available event type. For each search, we crawl through the results and gather all the urls for the spaces. A progress update is printed every 30 mins detailing the number of event type searches that have been completed and an approximate total number of results pages scraped.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10f238c-eeee-4bed-a6fa-d969941c5ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore, left in for testing / debugging purposes\n",
    "#event_types = event_types[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92922900-bc07-4581-b215-ad0cefb9f370",
   "metadata": {},
   "outputs": [],
   "source": [
    "space_urls = []\n",
    "time_last_update = time.time()\n",
    "total_event_types = len(url_event_types)\n",
    "# Loop through event types \n",
    "for event_number, event_type in enumerate(url_event_types): \n",
    "    # Provide progress report periodically  \n",
    "    if (time.time() - time_last_update > progress_report_interval):\n",
    "        print(f\"Scraped {event_number} of {total_event_types} event_types\")\n",
    "        # Approximate number of pages scraped is total number of urls \n",
    "        # found divided by the max results per page (36)\n",
    "        pages_of_urls_scraped = len(space_urls)/36\n",
    "        print(f\"Approximately {pages_of_urls_scraped:.0f} pages of search results scraped\\n\")\n",
    "        time_last_update = time.time()\n",
    "    # Create initial search url for current event type i.e. page 1 \n",
    "    # of search results \n",
    "    search_url = create_search_url(event_type, 1)\n",
    "    load_page(search_url)\n",
    "    total_pages = find_total_results_pages()\n",
    "    # Loop the current page number, from 1 to the total number \n",
    "    # of pages in search results \n",
    "    for current_page in range(1, total_pages + 1):\n",
    "        # Extract space urls from current page of search results\n",
    "        space_urls.extend(get_space_urls())\n",
    "        # If not on final page of search results\n",
    "        if(current_page < total_pages):\n",
    "            # Create new search url for the next page of \n",
    "            # search results (current page + 1) \n",
    "            search_url = create_search_url(event_type, current_page + 1)\n",
    "            load_page(search_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4956bcea-0a3d-4aaa-b4cf-492f8d051b63",
   "metadata": {},
   "source": [
    "### Scraping Space URLs Error Log\n",
    "If an error occurred whilst scraping the space urls it is displayed below. \n",
    "\n",
    "Note that if a page fails to load due to a connection issue, then it will result in a 'search url failed' error as well and you must re-scrape the entire search url that failed, not just the page that failed.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1275e5-a9dd-47a2-baaa-6ae912aefff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_errors = pd.DataFrame(scraping_error_log, columns = ['url','error'])\n",
    "# Function to make urls clickable in jupyter\n",
    "def make_clickable(val):\n",
    "    return '<a href=\"{}\">{}</a>'.format(val,val)\n",
    "\n",
    "scrape_errors.style.format({'url': make_clickable})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2c24c8-8ed8-457e-8862-3355427f3845",
   "metadata": {},
   "source": [
    "### Cleaning Space URLs\n",
    "\n",
    "Below we remove duplicates from the space urls and save the data to file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c109dc-6012-46df-b984-56a0eb262a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate space urls \n",
    "space_urls_unique = list(set(space_urls))\n",
    "print(f\"There are {len(space_urls_unique)} unique space urls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c824c3-7b35-4717-aac0-539d89bf2a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save space_urls_uniqe to file (as json)\n",
    "with open(\"space_urls.json\", 'w') as f:\n",
    "    # indent=2 is not needed but makes the file human-readable\n",
    "    json.dump(space_urls_unique, f, indent=2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe711b02-af93-4135-a8d4-20413f33ca48",
   "metadata": {},
   "source": [
    "If you wish to load a saved list of space urls, remove #s and run the below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f42d7cd-1834-40e1-985a-f19ec05e902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"space_urls.json\", 'r') as f:\n",
    "#    space_urls_unique = json.load(f)\n",
    "\n",
    "#print(f\"There are {len(space_urls_unique)} spaces to scrape\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cecc59-41f6-440b-af89-29acc12c5711",
   "metadata": {},
   "source": [
    "Every space web page has a standard url format, an example is shown below: \n",
    "https://www.tagvenue.com/rooms/london/6637/the-goldsmiths-centre/bench\n",
    "\n",
    "By exploring the space urls we scraped, we found that sometimes the above format was tweaked by adding extra text to the end of it. This text had the form '?event-offer=offer' where 'offer' varied, and could be 'Christmas' or 'Wedding' or some other value. \n",
    "\n",
    "Below we show an example of a tweaked url:  \n",
    "https://www.tagvenue.com/rooms/london/6637/the-goldsmiths-centre/bench?event-offer=christmas\n",
    "\n",
    "Some web pages have a 'event offers and packages' section which lists one or more special packages available at the venue. The tweaked url would typically link to the same web page as the normal format url but would auto-expand a special package defined by the extra bit of url e.g. the extra bit of url '?event-offer=christmas' would auto-expand the Christmas package when the page loads.\n",
    "\n",
    "Occasionally, the tweaked url would open a slightly different web page for the space. All the key space data e.g. price, location, square footage etc. would be the same. The only difference is that a unique special package would be available in the 'event offers and packages' section that is not available on the normal web page. \n",
    "\n",
    "For our purposes, the special package data is not useful. We chose to remove all the tweaked url's because they were effectively duplicated links to the same space web page as the normal format url. \n",
    "\n",
    "Below, we perform a quick check to ensure all urls with a '?' are of the form we have investigated, namely '?event-offer='. This is to make sure we don't have other unusual url formats that need to be analysed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5443706e-34c2-41dc-a7f1-cc6b813bf42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_df = pd.DataFrame(space_urls_unique)\n",
    "\n",
    "# Find number of urls that include a '?' \n",
    "urls_with_qmark = urls_df[urls_df[0].str.contains('\\?')].shape[0]\n",
    "# Find number of urls that include '?event-offer='\n",
    "urls_with_qmark_event_offer = urls_df[urls_df[0]\n",
    "                                      .str.contains('\\?event-offer')].shape[0]\n",
    "# Check all strings with '?' are of format '?event-offer='\n",
    "if (urls_with_qmark == urls_with_qmark_event_offer): \n",
    "    print(f\"All urls with a '?' are of form '?event-offer'\" )\n",
    "else: \n",
    "    print(f\"There are urls with '?' not of the form '?event-offer'\")\n",
    "\n",
    "# Remove urls that contain '?event-offer=' and save as list\n",
    "space_urls_cleaned = urls_df[~urls_df[0]\n",
    "                             .str.contains('\\?event-offer')][0].to_list()\n",
    "                 \n",
    "print(f\"Removed {urls_with_qmark_event_offer} urls containing '?event-offer' \\n\")\n",
    "print(f\"There are now {len(space_urls_cleaned)} space urls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413e74b8-f934-4932-ad2a-a652d92186d0",
   "metadata": {},
   "source": [
    "Below we display any urls that contain a '?' but don't contain 'event-offer' (this should show nothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01e0c41-5b93-4a52-b8c5-bf99fbad6ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view dataframe contains all urls with a '?' that do not \n",
    "# include the words 'event-offer'\n",
    "view = urls_df[(urls_df[0].str.contains('\\?'))\n",
    "               &(~urls_df[0].str.contains('event-offer'))]\n",
    "# Display view dataframe with clickable url links \n",
    "view.style.format({0: make_clickable})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7921434e-a375-4db2-8e52-91103943bcd0",
   "metadata": {},
   "source": [
    "The below code is useful for searching through the venue urls to see the different venues and to find different packages available at the venues. It was used to investigate the '?event-offer' urls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc8e9c2-cef3-4bb8-9f11-d37ef86a8974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch between space_urls_unique and space_urls_cleaned to \n",
    "# get with and without packages, and to find specific venues \n",
    "# or venues with packages e.g. search for '\\?event-offer=wedding' \n",
    "# to get venues with wedding packages\n",
    "urls_df = pd.DataFrame(space_urls_unique)\n",
    "# Filter dataframe by string inclusion\n",
    "view = urls_df[urls_df[0].str.contains('tanner')]\n",
    "# view urls with urls as clickable links \n",
    "view.style.format({0: make_clickable})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd1991e-ca78-4e82-b2cc-c0a5d5ce1c53",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Scrape Space Webpage HTML\n",
    "We will now scrape the full html code from every space web page. We chose to scrape the full html code rather than scraping individual elements that we need. This decision was made because the html elements tend to be inconsistent between web pages. Anticipating and handling all the errors and issues that might crop up is very challenging. The scraping process itself is very slow because time is waited between successive page loads. This means when an unexpected error occurs during the scrape we need to restart the whole process. Pages that failed to scrape or didnt scrape as intended need to be re-loaded and re-scraped.     \n",
    "It is much simpler, faster and less error prone to download the full html and then extract what we want separately.\n",
    "\n",
    "Tagvenue includes an overall user score for each venue. This score is displayed on each space web page for for that venue. You can click on 'Read all' next to the score to see a breakdown of the score into several categories e.g. 'Communication', 'Value' etc. When the web page of a space is initially loaded, it's html does not include these user score breakdowns. We wished to include them in the scrape. As such, before the space's html is saved, Selenium clicks on 'Read all' and loads the user score breakdown data.     \n",
    "\n",
    "Below we scrape the full html code from all space web pages that we have a url for. This includes a periodic progress report showing the number and percentage of pages that have been scraped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c08a10-41a1-48ae-8ab7-16761cccff63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def click_read_all():\n",
    "    \"\"\"Clicks 'Read all' button to load breakdown of venue review score.  \n",
    "    \n",
    "    Each venue has an overall user review score. By clicking on \n",
    "    'Read all' you can see a breakdown of the overall review score \n",
    "    into 6 different scoring categories e.g. 'Catering', \n",
    "    'location', 'Value' etc. The function waits until the page has\n",
    "    finished loading to ensure all data has loaded before scraping the html\"\"\"\n",
    "    try:\n",
    "        # Finds 'Read all' button html element\n",
    "        read_all_element = driver.find_element_by_xpath(\n",
    "            \"//button[@class='c-button-link' and contains(text(),'Read all')]\")\n",
    "        read_all_element.click()\n",
    "    except NoSuchElementException:\n",
    "        # End function, no reviews so no 'Read all' button to click\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Wait until page finishes loading following the click \n",
    "        WebDriverWait(driver, 10).until(\n",
    "            lambda d: d.execute_script('return document.readyState') == 'complete'\n",
    "        )\n",
    "    # If page fails to finish loading after 10 seconds, log error \n",
    "    except TimeoutException: \n",
    "        scraping_error_log.append([url, 'failed to load user review score breakdowns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abee9f56-7bdc-4ae9-96a2-8ddfec464a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary storing the url and html of each space webpage, in \n",
    "# format {url:html} \n",
    "space_webpages = {}\n",
    "total_urls_to_scrape = len(space_urls_cleaned)\n",
    "time_last_update = time.time()\n",
    "\n",
    "# Loop through space urls and scrape the html code for each space web page\n",
    "for url_number, url in enumerate(space_urls_cleaned):\n",
    "    # load space url, if page load successful (returns True) then go \n",
    "    # into 'if' code \n",
    "    if (load_page(url)):\n",
    "        click_read_all()\n",
    "        # Save space html code in space_webpages\n",
    "        space_webpages[url] = driver.page_source\n",
    "    # Periodic progress update \n",
    "    if (time.time() - time_last_update > progress_report_interval):\n",
    "        perc_complete = url_number / total_urls_to_scrape\n",
    "        print(f\"Scraped {url_number} of {total_urls_to_scrape} -> {perc_complete:.1%} Completed\\n\")\n",
    "        time_last_update = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dba648e-9760-491c-90e0-707083a45216",
   "metadata": {},
   "source": [
    "### Errors During Space HTML Scraping\n",
    "If an error occurred whilst scraping the space html code, it is displayed below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1c3874-ed10-4cf8-a4c2-d0de7460d6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_errors = pd.DataFrame(scraping_error_log, columns = ['url','error'])\n",
    "# Function to make urls clickable in jupyter\n",
    "def make_clickable(val):\n",
    "    return '<a href=\"{}\">{}</a>'.format(val,val)\n",
    "\n",
    "scrape_errors.style.format({'url': make_clickable})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bffc4a0-c75f-4583-84d3-eae0b2c9e9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save space webpage html data to file \n",
    "with open('space_htmls.json', 'w') as fp:\n",
    "    json.dump(space_webpages, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72349ae-ec67-47b7-907c-8687f7190825",
   "metadata": {},
   "source": [
    "Uncomment the below code if you wush to load a saved space_htmls file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a072bc39-0aef-412b-a861-a8509bf26c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('space_htmls.json', 'r') as fp:\n",
    "    space_webpages = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dde0a7-b4d2-4b4a-aaa8-93b4091d42d8",
   "metadata": {},
   "source": [
    "### Extract Data From HTML\n",
    "We will use the lxml library to extract the data we want from the html code of each space's website. The extraction code below loops through each space's web page html and extracts the data. \n",
    "\n",
    "Some spaces were no longer hosted by Tagvenue and therefore an error page was loaded into chrome when scraping their html rather than a Space web page. To handle these instances, the extraction code checks whether the title of the html page starts with '404' - if so the venue is no longer hosted and the extraction is aborted. A 'venue no longer hosted' error is then logged. If any other unexpected error occurs during extraction, the extraction is also aborted and the error message is logged. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31b96961-3d0d-46db-95c6-74db34571d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error log for extraction errors\n",
    "extraction_error_log = []\n",
    "\n",
    "def extract_data(url, html):\n",
    "    \"\"\"Attempts to extract data from the space url and html provided. \n",
    "    \n",
    "    First, the function checks if the space is no longer hosted by Tagvenue. \n",
    "    If it is hosted, the url and html are passed to the extract_from_html \n",
    "    function which returns the extracted data. If an error occurs during \n",
    "    extraction, the error is logged. \n",
    "    \n",
    "    Returns - Extracted data as a list or returns None if unable to \n",
    "    extract data due to venue no longer being hosted, or due to an \n",
    "    unexpected error. \n",
    "    \"\"\"\n",
    "    # check if the venue is no longer hosted\n",
    "    if (check_page_not_hosted(html)):\n",
    "        # Log extraction error due to venue no longer hosted \n",
    "        extraction_error_log.append([url, 'venue no longer hosted', html])\n",
    "        return None\n",
    "    try:\n",
    "        # Extract the data from the html code\n",
    "        return extract_from_html(url, html)\n",
    "    except Exception as e:\n",
    "        # Log unexpected error, including error message in log\n",
    "        extraction_error_log.append([url, e, html])\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5898365-df88-4fda-a19d-eadca769d2be",
   "metadata": {},
   "source": [
    "The lxml html extraction method always returns a list of results and doesnt raise an error if no elements are found. We prefer the Selenium methods which allow you to return either a list of results or a single result and return an error if it is not found. Below we defined some custom functions to make the lxml extraction methods function the same as the selenium find_elements_by_xpath and find_element_by_xpath methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "492c8fda-4fad-488d-b476-b2b54ad6de97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_element_by_xpath(xpath):\n",
    "    \"\"\"Returns first element found using provided xpath. \n",
    "    \n",
    "    If no element is found, an error is raised\"\"\"\n",
    "    # Finds all elements from provided Xpath as a list\n",
    "    elements = tree.xpath(xpath)\n",
    "    try:\n",
    "        return elements[0]\n",
    "    except IndexError:\n",
    "        raise IndexError('No element found via provided Xpath')\n",
    "    \n",
    "def find_elements_by_xpath(xpath):\n",
    "    \"\"\"Returns all elements found via provided xpath as a list\"\"\"\n",
    "    return tree.xpath(xpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afdaad98-7e01-4e69-b616-f964958b9ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_html(url, html):\n",
    "    \"\"\"Extracts and returns data from provided html.\"\"\"\n",
    "    # Make tree global so that custom lxml find html element functions \n",
    "    # work without having to pass the tree as an argument \n",
    "    global tree\n",
    "    # Parse html with lxml library\n",
    "    tree = lxml.html.fromstring(html)\n",
    "    \n",
    "    # Find h1 header html element that contains venue and space name \n",
    "    header_element = find_element_by_xpath(\"//h1\")\n",
    "    # Extract space and venue name string from html element\n",
    "    header = header_element.get('title').lower()\n",
    "    # header has general form 'space_name at venue_name'. Split venue\n",
    "    # and space name into a list using ' at ' as separator. \n",
    "    venue_and_space_name = header.split(' at ')\n",
    "    space_name = venue_and_space_name[0]\n",
    "    venue_name = venue_and_space_name[1]\n",
    "    # If list has more than 2 elements, an issue has occured\n",
    "    if (len(venue_and_space_name) > 2):\n",
    "        # Take venue and space name from url. End of url has \n",
    "        # format /venue_name/space_name. We split the url \n",
    "        # into a list with separator '/'\n",
    "        url_split = url.split('/')\n",
    "        # Last element of url split is space_name\n",
    "        space_name = url_split[-1].replace('-',' ')\n",
    "        # Second last element of url split is venue_name\n",
    "        venue_name = url_split[-2].replace('-',' ')\n",
    "\n",
    "    # Find address html element\n",
    "    address_element = find_element_by_xpath(\n",
    "        \"//span[@class='c-room-header__text_link' and contains(text(),',')]\")\n",
    "    # Extract text, remove '\\n's and whitespace\n",
    "    address = address_element.text.replace('\\n','').strip()\n",
    "    \n",
    "    # Find html element for map\n",
    "    map_element = find_element_by_xpath(\"//a[@href='#map-modal']\")\n",
    "    # Extract latitude and convert to float\n",
    "    latitude = float(map_element.get('data-lat'))\n",
    "    # Extract longitude and convert to float\n",
    "    longitude = float(map_element.get('data-long'))\n",
    "    \n",
    "    # Extract nearest tube station text and remove whitespace \n",
    "    try:\n",
    "        nearest_tube_station = find_element_by_xpath(\n",
    "        \"//div[@class='c-room-header__transport \"\n",
    "        + \"js-open-map-modal']//span/text()\").strip()\n",
    "    except:\n",
    "        nearest_tube_station = np.nan\n",
    "    \n",
    "    # Max seated and max standing data is not always present. when it is\n",
    "    # not present, it means the respective max value is 0. We will set\n",
    "    # both values to 0 initially and update with the extracted value if \n",
    "    # it can be found.  \n",
    "    max_seated = 0\n",
    "    max_standing = 0 \n",
    "    \n",
    "    # Extract list of capacity data. It has typical format [max seated,\n",
    "    # max standing, area of venue] but will have fewer elements if max\n",
    "    # seating or max standing is not available.  \n",
    "    capacity_data = find_elements_by_xpath(\n",
    "        \"//a[@href='#capacitySection']//strong/text()\")\n",
    "    \n",
    "    # Extracts the html elements for the capacity data, one element \n",
    "    # each for max standing, max sitting and area.\n",
    "    capacity_data_text = find_elements_by_xpath(\n",
    "        \"//a[@href='#capacitySection']//div[@class='c-venue-feature__label']\")\n",
    "    # Extract text from each of the capacity html elements - this text\n",
    "    # inlcudes strings that indicate if the value is for standing or \n",
    "    # sitting etc. We need to use this text to determine what max \n",
    "    # value is contained in each element because sometimes some of \n",
    "    # the elements are missing, so we dont know the index for \n",
    "    # seating or standing in that case.  \n",
    "    capacity_data_text = [element.text_content().strip() \n",
    "                          for element \n",
    "                          in capacity_data_text]\n",
    "    \n",
    "    # Loop through capacity text and data, determine what value the \n",
    "    # each data element refers to by checking whether the text \n",
    "    # contains the words 'seats', 'standing' or 'm' and then record that value\n",
    "    for text, data in zip(capacity_data_text, capacity_data):\n",
    "        if 'seats' in text:\n",
    "            max_seated = int(data)\n",
    "        if 'standing' in text:\n",
    "            max_standing = int(data)\n",
    "        if 'm' in text:\n",
    "            area_in_m2 = int(data[:-2])\n",
    "    \n",
    "    # Get list of venue catering data [catering_offered, external_catering]\n",
    "    venue_catering_data = find_elements_by_xpath(\n",
    "        \"//a[@href='#cateringSection']/div/div[@class='c-\" \n",
    "        + \"venue-feature__label']/text()\")\n",
    "    \n",
    "    catering_offered = venue_catering_data[0].strip()\n",
    "    external_catering_allowed = venue_catering_data[1].strip()\n",
    "    \n",
    "    return [url, venue_name, space_name, latitude, longitude, address, \n",
    "            nearest_tube_station, max_seated, max_standing, area_in_m2,\n",
    "            catering_offered, external_catering_allowed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f67ee32e-1933-4fbc-b9a1-e882123fcaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#driver.get('https://www.tagvenue.com/rooms/london/4472/bma-house/aldrich-blake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5597b413-8e1b-43cc-81c4-df2a73078856",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import lxml.html\n",
    "#tree = lxml.html.fromstring(space_webpages['https://www.tagvenue.com/rooms/london/4472/bma-house/aldrich-blake'])\n",
    "#find_element_by_xpath(\"//h1\").get('title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bc96872-f357-4f16-8496-cbe565e3268c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_page_not_hosted(html): \n",
    "    \"\"\"Check if venue no longer hosted, return result (True or False)\"\"\"\n",
    "    # Make tree global so that custom lxml find html element functions \n",
    "    # work without having to pass the tree as an argument \n",
    "    global tree\n",
    "    # Parse html with lxml library\n",
    "    tree = lxml.html.fromstring(html)\n",
    "    # Find text of title\n",
    "    title = find_element_by_xpath(\"//title/text()\")\n",
    "    # When not hosted, Tagvenue returns a 404 error at the \n",
    "    # beggining of the html title  \n",
    "    if (title[0:3] == '404'):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22f2bd77-8640-4248-8cd9-c5fcbd08524a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore - used for testing and debugging\n",
    "# space_webpages_test = {url:html for url, html in list(space_webpages.items())[0:10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb2c4381-7bd2-495c-bb24-8f41f398c80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4344 spaces were successfully scraped\n"
     ]
    }
   ],
   "source": [
    "# Extract data from the space webpages that were scraped\n",
    "data = [extract_data(url, html) for url, html in space_webpages.items()]\n",
    "# Remove None entries in data (These are from the erroneous \n",
    "# web pages that couldn't be scraped)\n",
    "data = [item for item in data if (item != None)]\n",
    "print(f\"{len(data)} spaces were successfully scraped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a2f186-4b7d-404b-9911-26cab49c56d8",
   "metadata": {},
   "source": [
    "### Extraction Error Log\n",
    "If extraction failed on a webpage, display error below along with error message. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2fdc9c4-1e8d-4922-8624-39523e9ae584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_90b78_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >url</th>\n",
       "      <th class=\"col_heading level0 col1\" >error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x10f179c10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraction_errors = pd.DataFrame(extraction_error_log, columns = ['url','error', 'html'])\n",
    "# Function to make urls clickable in jupyter\n",
    "def make_clickable(val):\n",
    "    return '<a href=\"{}\">{}</a>'.format(val,val)\n",
    "\n",
    "extraction_errors[['url','error']].style.format({'url': make_clickable})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5f1e2e-6195-45d8-a849-b009d8f71e68",
   "metadata": {},
   "source": [
    "### Summarise and Save Data\n",
    "Below we convert the extracted data to a dataframe, show first 10 rows and summary statistics. We then save the data to file as a csv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b183875-12c2-467d-b288-757709a42e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>venue_name</th>\n",
       "      <th>space_name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>address_line</th>\n",
       "      <th>nearest_tube_station</th>\n",
       "      <th>max_seated</th>\n",
       "      <th>max_standing</th>\n",
       "      <th>area_in_m2</th>\n",
       "      <th>catering_offered</th>\n",
       "      <th>external_catering_allowed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.tagvenue.com/rooms/london/10613/th...</td>\n",
       "      <td>the vaults</td>\n",
       "      <td>the mezzy one</td>\n",
       "      <td>51.502004</td>\n",
       "      <td>-0.115753</td>\n",
       "      <td>Leake Street, London, SE1 7NN</td>\n",
       "      <td>Waterloo Station (250 yd)</td>\n",
       "      <td>150</td>\n",
       "      <td>350</td>\n",
       "      <td>208</td>\n",
       "      <td>Venue offers catering</td>\n",
       "      <td>External catering not allowed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.tagvenue.com/rooms/london/4472/bma...</td>\n",
       "      <td>bma house</td>\n",
       "      <td>aldrich-blake</td>\n",
       "      <td>51.525795</td>\n",
       "      <td>-0.128939</td>\n",
       "      <td>BMA House,  Tavistock Square , London , WC1H 9JP</td>\n",
       "      <td>Euston Station (500 yd)</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>Venue offers catering</td>\n",
       "      <td>External catering allowed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.tagvenue.com/rooms/london/6003/goo...</td>\n",
       "      <td>good hotel london</td>\n",
       "      <td>living space</td>\n",
       "      <td>51.507978</td>\n",
       "      <td>0.018974</td>\n",
       "      <td>Western Gateway, London, E16 1FA</td>\n",
       "      <td>Royal Victoria Station (150 yd)</td>\n",
       "      <td>70</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>Venue offers catering</td>\n",
       "      <td>External catering not allowed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.tagvenue.com/rooms/london/9994/rad...</td>\n",
       "      <td>radisson blu edwardian heathrow</td>\n",
       "      <td>royal d</td>\n",
       "      <td>51.480911</td>\n",
       "      <td>-0.440963</td>\n",
       "      <td>Bath Road, 140, London, UB3 5AW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>60</td>\n",
       "      <td>75</td>\n",
       "      <td>Venue offers catering</td>\n",
       "      <td>External catering not allowed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.tagvenue.com/rooms/london/3071/bub...</td>\n",
       "      <td>bubba gump shrimp</td>\n",
       "      <td>exclusive use</td>\n",
       "      <td>51.510322</td>\n",
       "      <td>-0.132196</td>\n",
       "      <td>13 Coventry Street, , London, W1D 7AB</td>\n",
       "      <td>Piccadilly Circus Station (150 yd)</td>\n",
       "      <td>400</td>\n",
       "      <td>600</td>\n",
       "      <td>600</td>\n",
       "      <td>Venue offers catering</td>\n",
       "      <td>External catering not allowed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.tagvenue.com/rooms/london/5956/esq...</td>\n",
       "      <td>esquires coffee balham</td>\n",
       "      <td>whole coffee shop</td>\n",
       "      <td>51.442915</td>\n",
       "      <td>-0.149593</td>\n",
       "      <td>75 Bedford Hill, London, SW12 9HA</td>\n",
       "      <td>Balham Station (300 yd)</td>\n",
       "      <td>60</td>\n",
       "      <td>80</td>\n",
       "      <td>167</td>\n",
       "      <td>Venue offers catering</td>\n",
       "      <td>External catering allowed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://www.tagvenue.com/rooms/london/25304/go...</td>\n",
       "      <td>go ape alexandra palace</td>\n",
       "      <td>treetop adventure</td>\n",
       "      <td>51.596848</td>\n",
       "      <td>-0.129395</td>\n",
       "      <td>Alexandra Palace Way, London, N22 7AY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>Venue doesn’t offer catering</td>\n",
       "      <td>External catering allowed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://www.tagvenue.com/rooms/london/21639/sc...</td>\n",
       "      <td>scale space</td>\n",
       "      <td>boardroom</td>\n",
       "      <td>51.512439</td>\n",
       "      <td>-0.221433</td>\n",
       "      <td>Wood Lane, 58, London, W12 7RZ</td>\n",
       "      <td>White City Station (250 yd)</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>Venue doesn’t offer catering</td>\n",
       "      <td>External catering allowed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://www.tagvenue.com/rooms/london/15409/th...</td>\n",
       "      <td>the upper norwood library hub</td>\n",
       "      <td>the burgundy room</td>\n",
       "      <td>51.419844</td>\n",
       "      <td>-0.081249</td>\n",
       "      <td>39 - 42 Westow Hill, Upper Norwood, London, SE...</td>\n",
       "      <td>Crystal Palace Station (850 yd)</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>129</td>\n",
       "      <td>Venue offers catering</td>\n",
       "      <td>External catering not allowed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://www.tagvenue.com/rooms/london/14967/bo...</td>\n",
       "      <td>boston manor road</td>\n",
       "      <td>paragon suite</td>\n",
       "      <td>51.489400</td>\n",
       "      <td>-0.314441</td>\n",
       "      <td>Boston Manor Road, London, TW8 9GB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>70</td>\n",
       "      <td>326</td>\n",
       "      <td>Venue offers catering</td>\n",
       "      <td>External catering not allowed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://www.tagvenue.com/rooms/london/10613/th...   \n",
       "1  https://www.tagvenue.com/rooms/london/4472/bma...   \n",
       "2  https://www.tagvenue.com/rooms/london/6003/goo...   \n",
       "3  https://www.tagvenue.com/rooms/london/9994/rad...   \n",
       "4  https://www.tagvenue.com/rooms/london/3071/bub...   \n",
       "5  https://www.tagvenue.com/rooms/london/5956/esq...   \n",
       "6  https://www.tagvenue.com/rooms/london/25304/go...   \n",
       "7  https://www.tagvenue.com/rooms/london/21639/sc...   \n",
       "8  https://www.tagvenue.com/rooms/london/15409/th...   \n",
       "9  https://www.tagvenue.com/rooms/london/14967/bo...   \n",
       "\n",
       "                        venue_name         space_name   latitude  longitude  \\\n",
       "0                       the vaults     the mezzy one   51.502004  -0.115753   \n",
       "1                        bma house      aldrich-blake  51.525795  -0.128939   \n",
       "2                good hotel london      living space   51.507978   0.018974   \n",
       "3  radisson blu edwardian heathrow            royal d  51.480911  -0.440963   \n",
       "4                bubba gump shrimp      exclusive use  51.510322  -0.132196   \n",
       "5           esquires coffee balham  whole coffee shop  51.442915  -0.149593   \n",
       "6          go ape alexandra palace  treetop adventure  51.596848  -0.129395   \n",
       "7                      scale space          boardroom  51.512439  -0.221433   \n",
       "8    the upper norwood library hub  the burgundy room  51.419844  -0.081249   \n",
       "9                boston manor road     paragon suite   51.489400  -0.314441   \n",
       "\n",
       "                                        address_line  \\\n",
       "0                      Leake Street, London, SE1 7NN   \n",
       "1   BMA House,  Tavistock Square , London , WC1H 9JP   \n",
       "2                   Western Gateway, London, E16 1FA   \n",
       "3                    Bath Road, 140, London, UB3 5AW   \n",
       "4              13 Coventry Street, , London, W1D 7AB   \n",
       "5                  75 Bedford Hill, London, SW12 9HA   \n",
       "6              Alexandra Palace Way, London, N22 7AY   \n",
       "7                     Wood Lane, 58, London, W12 7RZ   \n",
       "8  39 - 42 Westow Hill, Upper Norwood, London, SE...   \n",
       "9                 Boston Manor Road, London, TW8 9GB   \n",
       "\n",
       "                 nearest_tube_station  max_seated  max_standing area_in_m2  \\\n",
       "0           Waterloo Station (250 yd)         150           350        208   \n",
       "1             Euston Station (500 yd)          24             0         64   \n",
       "2     Royal Victoria Station (150 yd)          70           120         80   \n",
       "3                                 NaN          50            60         75   \n",
       "4  Piccadilly Circus Station (150 yd)         400           600        600   \n",
       "5             Balham Station (300 yd)          60            80        167   \n",
       "6                                 NaN          10            50        100   \n",
       "7         White City Station (250 yd)          16             0         33   \n",
       "8     Crystal Palace Station (850 yd)         100           100        129   \n",
       "9                                 NaN          50            70        326   \n",
       "\n",
       "               catering_offered      external_catering_allowed  \n",
       "0         Venue offers catering  External catering not allowed  \n",
       "1         Venue offers catering      External catering allowed  \n",
       "2         Venue offers catering  External catering not allowed  \n",
       "3         Venue offers catering  External catering not allowed  \n",
       "4         Venue offers catering  External catering not allowed  \n",
       "5         Venue offers catering      External catering allowed  \n",
       "6  Venue doesn’t offer catering      External catering allowed  \n",
       "7  Venue doesn’t offer catering      External catering allowed  \n",
       "8         Venue offers catering  External catering not allowed  \n",
       "9         Venue offers catering  External catering not allowed  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data, columns = ['url', 'venue_name', 'space_name', 'latitude',\n",
    "                                   'longitude', 'address', \n",
    "                                   'nearest_tube_station', 'max_seated', 'max_standing', \n",
    "                                   'area_in_m2', 'catering_offered', \n",
    "                                   'external_catering_allowed'])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc2e673e-7166-4c81-8f11-a2df2bd2ea42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>venue_name</th>\n",
       "      <th>space_name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>address_line</th>\n",
       "      <th>nearest_tube_station</th>\n",
       "      <th>max_seated</th>\n",
       "      <th>max_standing</th>\n",
       "      <th>area_in_m2</th>\n",
       "      <th>catering_offered</th>\n",
       "      <th>external_catering_allowed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4344</td>\n",
       "      <td>4344</td>\n",
       "      <td>4344</td>\n",
       "      <td>4344.000000</td>\n",
       "      <td>4344.000000</td>\n",
       "      <td>4344</td>\n",
       "      <td>3996</td>\n",
       "      <td>4344.000000</td>\n",
       "      <td>4344.000000</td>\n",
       "      <td>4344</td>\n",
       "      <td>4344</td>\n",
       "      <td>4344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>4344</td>\n",
       "      <td>1483</td>\n",
       "      <td>3069</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1472</td>\n",
       "      <td>816</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>538</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>https://www.tagvenue.com/rooms/london/10613/th...</td>\n",
       "      <td>radisson blu edwardian heathrow</td>\n",
       "      <td>whole venue</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bath Road, 140, London, UB3 5AW</td>\n",
       "      <td>Piccadilly Circus Station (350 yd)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>Venue offers catering</td>\n",
       "      <td>External catering not allowed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>318</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38</td>\n",
       "      <td>36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>140</td>\n",
       "      <td>3683</td>\n",
       "      <td>3261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.509866</td>\n",
       "      <td>-0.124708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90.141805</td>\n",
       "      <td>128.754834</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.030561</td>\n",
       "      <td>0.076598</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>361.691642</td>\n",
       "      <td>432.586704</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.329423</td>\n",
       "      <td>-0.443729</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.501357</td>\n",
       "      <td>-0.146445</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.513125</td>\n",
       "      <td>-0.124229</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.521870</td>\n",
       "      <td>-0.085388</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.727560</td>\n",
       "      <td>0.187566</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      url  \\\n",
       "count                                                4344   \n",
       "unique                                               4344   \n",
       "top     https://www.tagvenue.com/rooms/london/10613/th...   \n",
       "freq                                                    1   \n",
       "mean                                                  NaN   \n",
       "std                                                   NaN   \n",
       "min                                                   NaN   \n",
       "25%                                                   NaN   \n",
       "50%                                                   NaN   \n",
       "75%                                                   NaN   \n",
       "max                                                   NaN   \n",
       "\n",
       "                             venue_name   space_name     latitude  \\\n",
       "count                              4344         4344  4344.000000   \n",
       "unique                             1483         3069          NaN   \n",
       "top     radisson blu edwardian heathrow  whole venue          NaN   \n",
       "freq                                 38          318          NaN   \n",
       "mean                                NaN          NaN    51.509866   \n",
       "std                                 NaN          NaN     0.030561   \n",
       "min                                 NaN          NaN    51.329423   \n",
       "25%                                 NaN          NaN    51.501357   \n",
       "50%                                 NaN          NaN    51.513125   \n",
       "75%                                 NaN          NaN    51.521870   \n",
       "max                                 NaN          NaN    51.727560   \n",
       "\n",
       "          longitude                     address_line  \\\n",
       "count   4344.000000                             4344   \n",
       "unique          NaN                             1472   \n",
       "top             NaN  Bath Road, 140, London, UB3 5AW   \n",
       "freq            NaN                               38   \n",
       "mean      -0.124708                              NaN   \n",
       "std        0.076598                              NaN   \n",
       "min       -0.443729                              NaN   \n",
       "25%       -0.146445                              NaN   \n",
       "50%       -0.124229                              NaN   \n",
       "75%       -0.085388                              NaN   \n",
       "max        0.187566                              NaN   \n",
       "\n",
       "                      nearest_tube_station    max_seated  max_standing  \\\n",
       "count                                 3996   4344.000000   4344.000000   \n",
       "unique                                 816           NaN           NaN   \n",
       "top     Piccadilly Circus Station (350 yd)           NaN           NaN   \n",
       "freq                                    36           NaN           NaN   \n",
       "mean                                   NaN     90.141805    128.754834   \n",
       "std                                    NaN    361.691642    432.586704   \n",
       "min                                    NaN      0.000000      0.000000   \n",
       "25%                                    NaN     15.000000     10.000000   \n",
       "50%                                    NaN     40.000000     60.000000   \n",
       "75%                                    NaN    100.000000    150.000000   \n",
       "max                                    NaN  20000.000000  20000.000000   \n",
       "\n",
       "       area_in_m2       catering_offered      external_catering_allowed  \n",
       "count        4344                   4344                           4344  \n",
       "unique        538                      2                              2  \n",
       "top           100  Venue offers catering  External catering not allowed  \n",
       "freq          140                   3683                           3261  \n",
       "mean          NaN                    NaN                            NaN  \n",
       "std           NaN                    NaN                            NaN  \n",
       "min           NaN                    NaN                            NaN  \n",
       "25%           NaN                    NaN                            NaN  \n",
       "50%           NaN                    NaN                            NaN  \n",
       "75%           NaN                    NaN                            NaN  \n",
       "max           NaN                    NaN                            NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f73970b-2ea7-40d2-9595-fc771b48fa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('tag_venue_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d8b172a-1365-408b-80aa-68f47fe3f2bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'driver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3z/dqb7xjp13pg6n8dk8d2gscy40000gn/T/ipykernel_22557/1653647873.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Close chrome page\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'driver' is not defined"
     ]
    }
   ],
   "source": [
    "# Close chrome page\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c49253-d65a-4032-8564-6e703da3faae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from operator import itemgetter\n",
    "local_vars = list(locals().items())\n",
    "# Size gives us variable size in Bytes\n",
    "size = [[var,sys.getsizeof(obj)] for var, obj in local_vars]\n",
    "size = sorted(size, key=itemgetter(1), reverse = True)\n",
    "for var, size in size:\n",
    "    print(var,f\"-> {size/1000000:,} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b5fb90-4c35-48c1-bf16-b9991dfbf6f6",
   "metadata": {},
   "source": [
    "### Investigating Extraction Errors\n",
    "Below provides examples of using the extraction_errors list to re-create the errors during extraction to aid with debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9154001-fa72-40cc-9ad3-2cdb505b46aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts error message from first error \n",
    "Error_number = 0\n",
    "repr(extraction_error_log[Error_number][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef63808-a451-4aef-aef8-8a7354007407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs extraction on erroneous html, returning full original error\n",
    "Error_number = 0\n",
    "extract_from_html('url', extraction_error_log[Error_number][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6501e4f-8124-4ab8-8ac3-25591e8df835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays the erroneous space web page html as html\n",
    "#HTML(space_webpages[extraction_error_log[Error_number][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2701eb-c9ed-4b11-b217-77ba85642a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_breakdown_title_elements = driver.find_elements_by_xpath(\n",
    "            \"//div[@class='reviews-modal-score__title' and contains(text(),'')]\")\n",
    "    review_breakdown_score_elements = driver.find_elements_by_xpath(\n",
    "            \"//div[@class='reviews-modal-score__score' and contains(text(),'')]\")\n",
    "    for element in review_breakdown_title_elements:\n",
    "        print(element.get_attribute('innerHTML'))\n",
    "        \n",
    "    for element in review_breakdown_score_elements:\n",
    "        print(element.get_attribute('innerHTML'))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e529aeba-1435-40e4-bd98-3cb9d2f2a933",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.C.str.join('|').str.get_dummies()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Venue_scrape",
   "language": "python",
   "name": "venue_scrape"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
